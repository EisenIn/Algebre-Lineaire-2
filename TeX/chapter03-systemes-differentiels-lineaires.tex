\chapter{Systèmes différentiels linéaires}
\label{cha:syst-diff-line}

On considère le système différentiel suivant
\begin{equation}
  \label{eq:19}
  \begin{array}{ccccc}
    \x'_1(t) & = &a_{11}\x_1(t) & + \cdots + & a_{1n}\x_n(t) \\
    \x'_2(t) & = &a_{21}\x_1(t) & + \cdots + & a_{2n}\x_n(t) \\
            & \vdots \\             
    \x'_n(t) & = &a_{n1}\x_1(t) & + \cdots + & a_{nn}\x_n(t)
  \end{array}
\end{equation}
où les $a_{ij} \in \R$.  
En notation matricielle, on peut écrire le système comme
\begin{displaymath}
  \x' = A\,\x
\end{displaymath}
où 
\begin{displaymath}
  A =
  \begin{pmatrix}
    a_{11} & \cdots & a_{1n}\\
          & \vdots & \\
          a_{n1} & \cdots & a_{nn}\\          
  \end{pmatrix}, \quad \x \in C^{1}(\R; \R^{n}).
\end{displaymath}
%
On cherche des fonctions dérivables $\x_i: \R \longrightarrow \R$  qui, ensemble, constituent $\x$ et qui  satisfont \eqref{eq:19}. Un tel $\x$ est une \emph{solution} du système~\eqref{eq:19}.
Soit $v ∈ ℝ^n$.  Une solution  $\x$   du système~\eqref{eq:19} respecte les \emph{conditions initiales} données par $v$,  si $x_i(0) = v_i$ pour $i=1,\dots,n$. On écrit  $\x(0) = v $. 



\begin{example}
  \label{exe:49}
  Considérons l'équation différentielle $\x'(t) = \x(t)$. Une solution est $\x(t) = e^t$. Une autre solution est $\x(t) = 2\cdot e^t$. Si on spécifie la \emph{condition initiale} $\x(0) = 1$, alors $\x(t) = e^t$ est l'unique solution qui satisfait cette condition initiale. Généralement, si on spécifie $\x(0) = \alpha$, alors  $\x(t) = \alpha \cdot e^t$ est la solution qui satisfait la condition initiale. 

Considérons $\x'(t) = -\x(t)$, une solution est $\x(t) = e^{(-t)}$. %$\x(t) = \cos(t)$.
C'est aussi une solution qui respecte la condition initiale $\x(0) = 1$. 
\end{example}

\begin{example}
  \label{exe:55}
  Soit
  $A =
  \begin{pmatrix}
    0 & -1\\
    1 & 0
  \end{pmatrix}$. On trouve deux solutions
  \begin{displaymath}
    \x_1(t) =  \begin{pmatrix}
      \cos{t}\\
      \sin{t}
    \end{pmatrix}
  \end{displaymath}
  et
  \begin{displaymath}
    \x_2(t) =   \begin{pmatrix}
      -       \sin{t} \\
      \cos{t}
    \end{pmatrix}
  \end{displaymath}
  et chaque combinaison linéaire des deux
  \begin{displaymath}
    α⋅ \x_1(t)  + β ⋅  \x_2 (t) 
  \end{displaymath}
  est une solution. Si on spécifie la condition initiale $\x(0) =
\left(  \begin{smallmatrix}
    1\\2
  \end{smallmatrix} \right)$ on cherche alors $α,β∈ℝ$ tel que
\begin{displaymath}
  \begin{pmatrix}
    1\\2
  \end{pmatrix} =
  α ⋅ x_1(0) + β ⋅x_2(0) = 
  \begin{pmatrix}
    1 & 0 \\
    0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    α \\ β
  \end{pmatrix}. 
\end{displaymath}
On met $α = 1$ et $β=2$, alors
$x_1(t) = \cos(t) -2 \sin(t)$ et $x_2(t) = \sin(t) + 2 \cos(t)$ est une solution, respectant les conditions initiales $\x(0) = \left(  \begin{smallmatrix}
    1\\2
  \end{smallmatrix} \right)$. 
\end{example}




Le théorème suivant est démontré en cours \emph{analyse 2}. 
\begin{theorem}[Cours d'analyse II] 
  \label{thr:28}
  Étant donné les \emph{conditions initiales} $\x(0)$
  il existe une unique solution $\x$ du système~\eqref{eq:19}. 
\end{theorem}
\noindent 
Nous sommes concernés par le problème de \emph{trouver} la solution $\x$ explicitement.



{\scriptsize   \color{lightgray}

\noindent   
  Essayons d'abord de résoudre le système en mettant  $\x(t) = e^{\lambda t} v$ où $v \in \R^n$ est un vecteur constant. Dans ce cas $\x' = A\x$ se récrit comme   $\lambda e^{\lambda t}  v = e^{\lambda t} A v$. Nous avons démontré le lemme suivant. \todo{ Partie en gris fait pas partie du cours 2024 } 

\begin{lemma}
  \label{thr:29}
  Si $\lambda \in \R$ est une valeur propre de $A$ et si $v \in \R^n \setminus \{0\}$ est un vecteur propre correspondant, alors $\x(t) = e^{\lambda t} v$ est  une solution du système~\eqref{eq:19} pour les conditions initiales $\x(0) = v$. 
\end{lemma}


 On commence avec une observation qui est un exercice simple. 

\begin{lemma}
  \label{lem:13}
  L'ensemble $\X = \{ \x \colon \x \text{ est une solution du système \eqref{eq:19}}\}$ est un espace vectoriel sur $\R$.  
\end{lemma}

Est-ce que c'est possible de donner  une base de $\X$ explicitement? Dans le cas où $A$ est diagonalisable : 
\begin{displaymath}
  A = P \cdot \diag(\lambda_1,\dots,\lambda_n) \cdot P^{-1} 
\end{displaymath}
où $P \in \R^{n \times n}$ est inversible et les  $\lambda_i$ sont réels, le théorème suivant décrit une base intuitive de $\X$.

\begin{theorem}
  \label{thr:30}
  Si $\R^n$ possède une base $\{v_1,\dots,v_n\} \subseteq \R^n$ de vecteurs propres de $A$ telle que $A \, v_i = \lambda_i v_i$, alors  
  \begin{displaymath}
    \x^{(i)}(t) = e^{\lambda_i t} \cdot v_i, \, i=1,\dots,n
  \end{displaymath}
est une base de $\X$. 
\end{theorem}

\begin{proof}
  Montrons d'abord que les $\x^{(i)}$ sont linéairement indépendants. Supposons que $\sum_{i} \alpha_i \x^{(i)} = 0$. C'est-à-dire que les $n$ fonctions qui sont les composantes de $\sum_{i} \alpha_i \x^{(i)}$ sont toutes la fonction identiquement nulle. En évaluant en $t=0$, on trouve
  \begin{displaymath}
    0 = \sum_i \alpha_i v_i e^{\lambda_i 0} = \sum_i \alpha_i v_i.  
  \end{displaymath}
Or les $v_i$ sont linéairement indépendants. On a donc $\alpha_i = 0$ pour tout $i$ ce qui démontre que les $\x^{(i)}$ sont linéairement indépendants. 


Maintenant soit $\y \in \X$ et soient  $\alpha_i \in \R$  tels que 
\begin{displaymath}
  \y(0) = \sum_i \alpha_i v_i.  
\end{displaymath}
Alors $\x := \sum_i \alpha_i \x^{(i)} \in \X$  et comme $\x(0) = \y(0)$, le Théorème~\ref{thr:28} implique que $\x = \y$. Les $\x^{(i)}$ engendrent donc $\X$, et $\{\x^{(1)},\dots, \x^{(n)}\}$ est une base de $\X$. 
\end{proof}


Est-ce qu'on peut aussi trouver une solution dans la cas où $A$ est diagonalisable dans les nombres complexes, donc si 
\begin{displaymath}
A=P \cdot \diag(\lambda_1,\dots,\lambda_n) \cdot P^{-1} 
\end{displaymath}
où $P \in \C^{n \times n}$ est inversible et les  $\lambda_i \in \C$? Pour discuter de ça, il faut d'abord définir, ce qu'est une solution complexe du système~\eqref{eq:19}. Toute fonction $f: \R \longrightarrow \C$ s'écrit comme 

\begin{displaymath}
  f(x) = f_{\Re}(x) + i \cdot f_{\Im}(x) 
\end{displaymath}
où $f_{\Re}(x), f_{\Im}(x)$ sont des fonctions de $ \R \longrightarrow \R$.  Si $f_\Re$ et $f_\Im$ sont dérivables, on dit que $f(x)$ est dérivable et on définit 
\begin{displaymath}
  f'(x) = f_\Re'(x) + i \cdot f_\Im'(x). 
\end{displaymath}
Si $\x_1,\dots,\x_n\colon \R \longrightarrow \C$ sont dérivables, comme avant 
\begin{displaymath}
  \x =
  \begin{pmatrix}
    \x_1\\ \vdots \\ \x_n
  \end{pmatrix}
\end{displaymath}
est une \emph{solution complexe} du système~\eqref{eq:19} si $\x' = A\x$.   Et comme avant, on peut noter le lemme suivant, en se rappellant que $e^{a + ib} = e^a (\cos b + i \cdot \sin b)$. 


\begin{lemma}
  \label{lem:15}
  Si $\lambda \in \C$ est une valeur propre de $A$ et si $v \in \C^n \setminus \{0\}$ est un vecteur propre correspondant, alors $\x(t) = e^{\lambda t} v$ est  une solution du système~\eqref{eq:19} pour les conditions initiales $\x(0) = v$. 
\end{lemma}
\begin{proof}
  On écrit 
  \begin{displaymath}
    \x' = \lambda e^{\lambda t} v = e^{\lambda t} Av = A\x.
  \end{displaymath}
\end{proof}


\begin{lemma}
  \label{lem:14}
  Étant donné une solution complexe $\x = \x_\Re + i \x_\Im$ du système~\eqref{eq:19}, alors $\x_\Re$ et $\x_\Im$ sont des solutions réelles.  
\end{lemma}
\begin{proof}
  Dès que $\x_\Re + i \x_\Im $ est une solution, on a 
  \begin{displaymath}
   \x'_\Re+ i \x'_\Im = \x' = A \x = A\x_\Re + i A\x_\Im. 
  \end{displaymath}
Comme $A $ est réelle on a $\x_\Re' = A\x_\Re$ et $\x'_\Im = A \x_\Im$ en prenant les parties réelles et imaginaires des deux côtés. 
\end{proof}


Supposons alors que $A \in \R^{n \times n}$ est diagonalisable . Et soit $\{v_1,\dots,v_n\}$ une base de $\C^n$ de vecteurs propres associés à 
$\lambda_1,\dots,\lambda_n$ respectivement. Si $v_i = u_i + i \cdot w_i$  où $u_i,w_i \in \R^n$, les $u_1,\dots,u_n,w_1,\dots,w_n$ engendrent $\R^n$ (voir exercice~\ref{item:5}). Comme nous avons noté 
\begin{displaymath}
  \x^{(j)} = e^{\lambda_j t} v_j
\end{displaymath}
sont des solutions complexes du système~\eqref{eq:19}.   

Aussi, on peut supposer que la base et les valeurs propres sont tels que les vecteurs/valeurs propres complexes viennent en paires conjugées complexes. Plus précisément, si $2k$ valeurs propres sont complexes et le reste sont réelles, on pose
\begin{equation}
\label{eq:22}
  v_{2j-1} = \overline{v_{2j}}\, \text{ et } \, \lambda_{2j-1} = \overline{\lambda_{2j}} \, \text{ pour } \, 1 \leq j \leq k \leq n/2 
\end{equation}
et 
\begin{equation}
  \label{eq:23}  
  v_j \in \R^n, \lambda_j \in \R \text{ pour } j > 2k. 
\end{equation}
%
Considérons maintenant une solution donnée par $v = u+iw$  $\lambda= a+ib$. 
\begin{eqnarray*}
  \x & = & e^{a \, t} \left(\cos (b t)  + i \sin (b t ) \right)  (u + i w)  \\
   & = & e^{a \, t} \left(\cos (b t) u - \sin (bt)w \right)  + ie^{a \, t} \left(\sin (b t ) u + \cos(bt)w \right). 
\end{eqnarray*}
Ceci nous donne alors deux solutions réelles 
\begin{eqnarray*}
  \x^{(1)} & = & e^{a \, t} \left(\cos (b t) u - \sin (bt)w \right), \\
  \x^{(2)} & = &  e^{a \, t} \left(\sin (b t ) u + \cos(bt)w \right). 
\end{eqnarray*}
\begin{remark}
  \label{rem:2}
  Les solutions réelles données par $v$ et $\lambda$ engendrent le même espace que les solutions réelles données par $\overline{v}$ et $\overline{\lambda}$. 
\end{remark}

Nous pouvons alors noter une marche à suivre pour résoudre le système~\eqref{eq:19} étant donné $\x(0)$ si $A$ est diagonalisable.
\begin{enumerate}
\item Trouver une base de vecteurs propres $v_1,\dots,v_n$ de $A$ ordonnée comme dans \eqref{eq:22} et \eqref{eq:23}. 
\item Pour chaque paire $v_{2j},\lambda_{2j}$, $1 \leq j \leq k$ trouver les  deux solutions réelles dénotées par  $\x^{(2j-1)}$ et $\x^{(2j)}$. 
\item Pour chaque paire réelle $v_j, \lambda_j$ $n\geq j>2k$, trouver la solution $\x^{(j)}$. 
\item Trouver la combinaison linéaire 
  \begin{displaymath}
    \x(0) = \sum_{j} \alpha_j \x^{(j)}(0)
  \end{displaymath}
\item La solution est 
  \begin{displaymath}
    \x = \sum_{j} \alpha_j \x^{(j)} 
  \end{displaymath}
\end{enumerate}



\begin{example}
  \label{exe:21}
  Résoudre le système $\x' = A\x$ où 
  \begin{displaymath}
    A  =
    \begin{pmatrix}
      1 & 2 \\
      -2 & 1
    \end{pmatrix} \, \text{ et } \, \x(0) =
    \begin{pmatrix}
      1\\1
    \end{pmatrix}. 
  \end{displaymath}
 On trouve que $\lambda_1 = 1 + 2 i$ et $\lambda_2 = 1 - 2i$ sont les valeurs propres de $A$ et 
 \begin{displaymath}
   v_1 =
   \begin{pmatrix}
     1\\i
   \end{pmatrix} \text{ et } v_2 =
   \begin{pmatrix}
     1 \\ -i
   \end{pmatrix}
 \end{displaymath}
sont les vecteurs propres correspondants. 
Les deux solutions impliquées par $v_1$ sont 
\begin{eqnarray*}
  \x^{(1)} & = & e^{ t} \left(\cos ( 2t)
                 \begin{pmatrix}
                   1\\0
                 \end{pmatrix}
- \sin (2t)\
  \begin{pmatrix}
    0\\1
  \end{pmatrix}
\right) \\
  \x^{(2)} & = &  e^{t} \left(\sin ( 2t )
                 \begin{pmatrix}
                   1\\0
                 \end{pmatrix}
+ \cos(2t)
  \begin{pmatrix}
    0\\1
  \end{pmatrix}
\right). 
\end{eqnarray*} 
La solution qu'on cherche est 
\begin{displaymath}
  \x = \begin{pmatrix}
           e^{t} \sin (2t) + e^{t} \cos(2t) \\
           - e^{t} \sin(2t) + e^{t} \cos(2t)
         \end{pmatrix}.
\end{displaymath}

\end{example}






% Une situation très agréable est si $A$ est diagonalisable. Soit $P^{-1}AP = \diag(\lambda_1,\dots,\lambda_n)$ où $P = (v_1,\dots,v_n)$. 
% Avec le changement de variables $\x = P\cdot \y $ on écrit
% \begin{displaymath}
%   \y' = P^{-1} \x' = P^{-1} A\x = P^{-1}AP\y = \diag(\lambda_1,\dots,\lambda_n) \y. 
% \end{displaymath}
% Le système 
% \begin{equation}
%   \label{eq:20}  
%   \y' = \diag(\lambda_1,\dots,\lambda_n) \y 
% \end{equation}
% est découplé et les conditions initiales sont $\y(0) = P^{-1} \x(0)$. La solution est $\y_i(t) = \y_i(0) e^{\lambda_i \cdot t}$ et $\x = P \cdot \y$ est la solution du système~\eqref{eq:19} pour les conditions initiales $\x(0)$. 

% \begin{remark}
%   \label{rem:1}
%   Notez que, même si $A$
%   est diagonalisable, seulement dans les nombre complexes, les
%   fonctions $\x$ sont réelles.
% \end{remark}


}

\subsection*{Exercices} 

\begin{enumerate}
\item Montrer Lemme~\ref{lem:13}. 
\item Une fonction $f:\C \longrightarrow \C$ est \emph{holomorphe} en $z_0 \in \C$ si
  \begin{displaymath}
    f'(z_0) = \lim_{z \rightarrow z_0} \frac{f(z) - f(z_0)}{z - z_0} 
  \end{displaymath}
existe. Soit $f$ holomorphe sur $\C$ et $g = f_{|\R}$ la fonction $f$ réduite à $\R$. 
 Montrer 
\begin{enumerate}[i)]
\item  $g(x) = g_\Re(x) + i \cdot g_\Im(x)$ est dérivable au  sens de notre définition, particulièrement $g_\Re(x)$ et $ g_\Im(x)$ sont dérivables. 
\item $f'_{| \R} (x) = g'_\Re(x) + i \cdot g'_\Im(x)$. 
\end{enumerate}
\item Soit $\{u_1+ i \cdot w_1,\dots,u_n + i \cdot w_n\}$ une base de $\C^n$ où $u_i,w_i \in \R^n$  pour tout $i$. Montrer que $\spa\{u_i, w_i \colon 1 \leq i \leq n\} = \R^n$. \label{item:5}
\end{enumerate}






\section{L'exponentielle d'une matrice}
\label{sec:lexp-dune-matr}


\begin{definition}
  \label{def:28}
  Pour $A \in \C^{n \times n}$ on définit 
  \begin{displaymath}
    e^A = I + A + \frac{1}{2!} A^2 + \frac{1}{3!}A^3 + \cdots 
  \end{displaymath}
\end{definition}

\noindent On rappelle la définition d'une série intégrable 
\begin{displaymath}
  \sum_{j=0}^\infty a_j z^j,
\end{displaymath}
où les coefficients $a_j \in\C$, et
qui converge sur un \emph{disque} de rayon $\rho$. C'est à dire que, si $|z|< \rho$ la série converge et la fonction $f\colon \{x \in \C \colon |x| < \rho \}  \rightarrow \C$ définie par $f(x) = \sum_{j=0}^\infty a_j x^j $ est \emph{holomorphe} avec dérivée $f'(x) =  \sum_{j=0}^\infty j a_j x^{j-1}$. 
Une série intégrable importante est la série
\begin{displaymath}
  e^{x} = \sum_{j=0}^\infty \frac{1}{j!} x^j,
\end{displaymath}
qui définit la fonction holomorphe $\exp: \C \longrightarrow \C$ 
\begin{displaymath}
  e^{a+i\,b} = e^a (\cos b + i \sin b), \quad   a,b ∈ ℝ. 
\end{displaymath}
%
\noindent On va maintenant généraliser la définition de la \emph{norme Frobenius} pour les matrices complexes. Pour $A \in \C^{m\times n}$, 
\begin{displaymath}
  \|A\|_F = \sqrt{\sum_{ij} |a_{ij}|^2 }. 
\end{displaymath}

\begin{lemma}
  \label{lem:16}
  Pour $A \in \C^{n \times m}$ et $B \in \C^{m × n}$  on a 
  \begin{displaymath}
    \|A\cdot B\|_F \leq \|A\|_F\cdot \|B\|_F. 
  \end{displaymath}
\end{lemma}

  \begin{proof}Soient $a_1^T,\dots,a_n^T \in \C^m$ les lignes de $A$ et $\overline{b_1},\dots,\overline{b_n} \in \C^m$ les colonnes de $B$. Avec Cauchy-Schwarz 
    \begin{displaymath}
          |(AB)_{ij}|^2 = (a_i^T \overline{b_j})(\overline{a_i}^T b_j)  \leq \|a_i\|^2 \|b_j\|^2
    \end{displaymath}
et donc 
\begin{displaymath}
  \|AB\|_F^2 = \sum_{ij} |(AB)_{ij}|^2 \leq \sum_i\|a_i\|^2 \cdot \sum_i \|b_i\|^2 = \|A\|_F^2 \cdot \|B\|_F^2. 
\end{displaymath}
  \end{proof}


  \begin{lemma}
    \label{lem:17}
    Soit $A ∈ ℂ^{n ×n}$. La série
    \begin{displaymath}
      e^A = I + A + \frac{1}{2!} A^2 + \frac{1}{3!}A^3 + \cdots 
    \end{displaymath}
    converge.  
  \end{lemma}

  \begin{proof}
%Il est facile de se convaincre que la convergence pour la norme de Frobenius revient à prouver que la suite est de Cauchy. 

%Evidemment $\forall x \in \mathbb{C}$ la série $\sum _{ j=0 }^{ \infty  }{ \frac { { x }^{ j } }{ j! } ={ e }^{ x } } $ converge. La suite des sommes partielles est donc de Cauchy.

    Considérons  la suite $(b_n)_{n\in \mathbb{N}} ∈ ℂ^{n ×n}$
    \begin{displaymath}      
      b_n = \sum _{j=0 }^{ n } \frac { { A }^{ j } }{ j! }  
    \end{displaymath}
%    
    On montre que cette suite  et une suite Cauchy par rapport la norme Frobenius $\|⋅\|_F$. 
    Soient $m≥n \in \mathbb{N}$, alors
    \begin{eqnarray*}
      \parallel b_{ m }-b_{ n }\parallel _{ F } & = & \parallel \sum _{ j=n+1 }^{ m }{ \frac { A^{ j } }{ j! }  } \parallel _{ F }\\
       & \le &  \sum_{j=n+1}^{m}{\frac{\parallel A^j\parallel_{F}}{j!}} \\ 
                                                & \le &  \sum_{j=n+1}^{m}{\frac{\parallel A\parallel_{F}^{j}}{j!}}.
                        %                        & \le &\frac{1}{n+1}   \sum_{j=n+1}^{m}{\frac{\parallel A\parallel_{F}^{j}}{j!}}
    \end{eqnarray*}
    Le première inégalité au dessus est l'inégalité triangulaire \emph{(Théorème~\ref{thr:1})} et la deuxième est Lemme~\ref{lem:16}.  
    Puisque la suite $(a_n)_{n\in \mathbb{N}} ∈ ℝ$
    \begin{displaymath}
      a_n = \sum_{j=0}^{n}{\frac{\parallel A\parallel_{F}^{j}}{j!}}
    \end{displaymath}
    es Cauchy  \emph{(Analyse 1)}, alors pour tout $ε>0$ il existe $N_ε ∈ ℕ$ tel que pour tout $m,n ≥ N_ε$
    \begin{displaymath}
      |a_m - a_n | < ε. 
    \end{displaymath}
    Alors, pour tous  $m,n ≥ N_ε$, on a
    \begin{displaymath}
      \parallel b_{ m }-b_{ n }\parallel _{ F }  < ε.  
    \end{displaymath}
    Ceci montre que $(b_n)$ est Cauchy et alors que $e^A$ converge. 
    % $\parallel b_{ m }-b_{ n }\parallel _{ F }=\parallel \sum _{ j=n+1 }^{ m }{ \frac { A^{ j } }{ j! }  } \parallel _{ F }\quad \le \quad \sum_{j=n+1}^{m}{\frac{\parallel A\parallel_{F}^{j}}{j!}} \le \quad \epsilon $ pour $n,m \ge N_{\epsilon}$ (qui existe car la suite des sommes partielles de la série $\sum _{ j=0 }^{ +\infty }{ \frac { \parallel A\parallel _{ F }^{ j } }{ j! }  } = e^{\parallel A\parallel _{ F }^{ j }} $ est de Cauchy.)
  \end{proof}

%Maintenant, soit $A ∈  ℝ^{n ×n}$. 
  Nous avons montré que $e^{At} = \sum_{k=0}^\infty \frac{t^k}{k!} A^k$ converge pour tout $t \in \R$. Plus précisément chaque composante $\sum_{k=0}^\infty \frac{t^k}{k!} A^k$ est une série intégrable avec un rayon de convergence $\infty$.
Nous pouvons donc dériver les termes de la somme pour obtenir 
\begin{equation}
  \label{eq:24}
  \frac{d}{dt} e^{At} = A e^{At}. 
\end{equation}


\begin{theorem}
  \label{thr:31}
  La solution du problème initial $\x' = A\x$, $\x(0) =v$ est 
  \begin{displaymath}
    \x(t) = e^{At} v.
  \end{displaymath}
\end{theorem}

\begin{proof}
  Soit $\x(t) = e^{At} v$. Alors $\x'(t) = A e^{At}v = A\x(t)$. Plutôt $\x(0) = v$. 
\end{proof}


\begin{example}[Exemple~\ref{exe:55} continué] 
  Soit encore 
  $A =
  \begin{pmatrix}
    0 & -1\\
    1 & 0
  \end{pmatrix}$ et les conditions initiales données par $v =  \left(\begin{smallmatrix}
    1\\2
  \end{smallmatrix}\right)$.
  On calcule 
  \begin{displaymath}
    e^{tA} =  I+tA+\dfrac{t^{2}A^{2}}{2}+\dfrac{t^{3}A^{3}}{3!}+\dots 
  \end{displaymath}
dans ce cas. 
Pour  $k∈ ℕ$ 
\begin{displaymath}
  A^{2k}=(-1)^{k} I  \quad \text{ et } \quad  A^{2k+1}=
\begin{pmatrix}
    0 & (-1)^{k+1}\\
    (-1)^{k} & 0
\end{pmatrix} 
\end{displaymath}
Alors, on a 
\begin{displaymath} 
  e^{tA} = 
  \begin{pmatrix} \displaystyle 
     \sum_{k=0}^{\infty}\dfrac{(-1)^{k}t^{2k}}{(2k)!} &  \displaystyle \sum_{k=0}^{\infty}\dfrac{(-1)^{k+1}t^{2k+1}}{(2k+1)!}\\
    \displaystyle \sum_{k=0}^{\infty}\dfrac{(-1)^{k+1}t^{2k+1}}{(2k+1)!} & \displaystyle \sum_{k=0}^{\infty}\dfrac{(-1)^{k}t^{2k}}{(2k)!}
\end{pmatrix}
\end{displaymath}
On se souvient des formules 
\begin{displaymath}
  \cos(t) =\displaystyle \sum_{k=0}^{\infty}\dfrac{(-1)^{k}t^{2k}}{(2k)!} \text{ et }\sin(t) =\displaystyle \sum_{k=0}^{\infty}\dfrac{(-1)^{k}t^{2k+1}}{(2k+1)!} 
\end{displaymath}
Alors
\begin{displaymath}
  e^{tA} = \begin{pmatrix}
    \cos(t)&-\sin(t)\\
    \sin(t)&\cos(t)
  \end{pmatrix}
\end{displaymath}
Pour  $v_0 = 
\begin{pmatrix}
    1\\
    2
\end{pmatrix} $ 
alors comme avant, 
\begin{displaymath}
  \x = \begin{pmatrix}
    \cos(t)&-\sin(t)\\
    \sin(t)&\cos(t)
  \end{pmatrix} \begin{pmatrix}
    1\\
    2
\end{pmatrix}   =  \begin{pmatrix}
    \cos(t)-2\sin(t)\\
    2\cos(t)+\sin(t)
\end{pmatrix} 
\end{displaymath}
est une solution respectons les conditions initiales  $v_0 = 
\begin{pmatrix}
    1\\
    2
\end{pmatrix} $. 
\end{example}






\begin{definition}
  \label{def:29}Soit $K$ un corps. 
  Une matrice $N ∈ K^{n ×n}$ est \emph{nilpotente} s'il existe un $k \in \N$ tel que $N^k = 0$. 
\end{definition}

Nous allons montrer ce théorème dans les prochaines cours.  
\begin{theorem}
  \label{thr:32}
  Chaque matrice $A \in \C^{n \times n}$ peut être factorisée comme 
  \begin{displaymath}
    A = P ( \diag(\lambda_1,\dots,\lambda_n) + N) P^{-1}
  \end{displaymath}
où $N \in \C^{n \times n}$ est nilpotente, $P \in \C^{n \times n}$ est inversible,  $\lambda_1,\dots,\lambda_n \in \C$ sont les valeurs propres de $A$ et $\diag(\lambda_1,\dots,\lambda_n)$ et $N$ commutent. 
\end{theorem}
%
On se souvient maintenant, comment montrer que $e^{x+y}  = e^xe^y$ pour $x,y∈ ℂ$.  Pour $N ∈ ℕ$, le produit des termes
$∑_{k=0}^N x^k / k! $ et $∑_{k=0}^N y^k / k! $ s'écrit comme 
\begin{eqnarray}
  \left(∑_{k=0}^N x^k / k! \right) \left(∑_{k=0}^N y^k / k! \right) & = & \displaystyle  ∑_{k=0}^{2N}  ∑_{i+j = k} \frac{x^i}{  i!} \frac{y^j}{ j!} \\
  & = &  \displaystyle ∑_{k=0}^{2N}  ∑_{j = 0}^k \binom{k}{j} \frac{x^{k-j} }{(k-j)!} \frac{y^{j} }{j!}.   
\end{eqnarray}
En fait, c'est le cas pour $x,y ∈ R$, où $R$ est un anneau et $x ⋅ y  = y ⋅x$, c.à.d. $x$ et $y$ commutent.  Dans ce cas, on a aussi
\begin{displaymath}
  \displaystyle  ∑_{kk=0}^{2N} \frac{(x+y)^j}{k!} =  \displaystyle  ∑_{k=0}^{2N}  ∑_{j = 0}^k \binom{k}{j} \frac{x^{k-j} }{(k-j)!} \frac{y^{j} }{j!}.   
\end{displaymath}
%
Basé sur cette observation, le lemme suivant est un exercice. 
\begin{lemma}
  \label{lem:18}
  Pour $A,B \in \C^{n \times n}$, si $A\cdot B = B \cdot A$ on a $e^{A+B} = e^A e^B$. 
\end{lemma}



Comment peut-on maintenant résoudre le problème initial
$\x' = Ax, \, \x(0) = v$ explicitement? Nous savons que cette solution
est $\x = e^{tA} \cdot v$ et nous savons que c'est une solution réelle
pour $A \in \R^{m \times n}$. Mais les premiers termes s'écrivent
comme
\begin{displaymath}
  \displaystyle \sum_{i=0}^m t^i A^i = P \left(\displaystyle \sum_{i=0}^m ( t \diag(\lambda_1,\dots,\lambda_n) + t N)^i / i!  \right) P^{-1}.  
\end{displaymath}
Maintenant,
\begin{displaymath}
  \sum_{i=0}^∞ ( t \diag(\lambda_1,\dots,\lambda_n) + t N)^i / i!  = e^{t⋅   \diag(\lambda_1,\dots,\lambda_n) + t N}. 
\end{displaymath}
Puisque $t⋅   \diag(\lambda_1,\dots,\lambda_n)$ et  $t⋅N$ commutent,  le théorème \ref{thr:32} implique
\begin{displaymath}
  e^{t⋅   \diag(\lambda_1,\dots,\lambda_n) + t N} = e^{t⋅   \diag(\lambda_1,\dots,\lambda_n)} ⋅e^{t N}
\end{displaymath}
La solution \emph{réelle} que l'on cherche est  alors 
\begin{eqnarray*}
  \x & = &  P e^{t\diag(\lambda_1,\dots,\lambda_n)} e^{tN} P^{-1} v \\
     & = & P \left( \diag(e^{\lambda_1 \, t},\dots,e^{\lambda_n \, t})\cdot  \displaystyle \sum_{j=0}^{k-1} t^j N^{j} / j!\right)P^{-1},
\end{eqnarray*}
où $k \in \N$ est tel que $N^k = 0$. 

\begin{example}[Exemple~\ref{exe:55} continué] 
  La matrice
  \begin{displaymath}
    A =
    \begin{pmatrix}
      0 & -1 \\
      1 & 0
    \end{pmatrix} 
  \end{displaymath}
  est diagonalisable sur $ℂ$.  En fait avec
  \begin{displaymath}
    P =  \left(\begin{array}{rr}
                 1 & 1 \\
                  i & - i
               \end{array}\right) \quad \text{ et }  \quad
             D = \left(\begin{array}{rr}
                         - i & 0 \\
                         0 &  i
                       \end{array}\right)
  \end{displaymath}
  on a 
  \begin{displaymath}
    A =
    P ⋅ D ⋅ P^{-1}.      
  \end{displaymath}
  Les premiers $N$ termes de la série $e^{tA}$, sont  alors
  \begin{eqnarray*}
    ∑_{k=0}^N t^k A^k / k!  & = & ∑_{k=0}^N t^k \left((P ⋅ D^k P^{-1}\right)/ k!   \\
                       & = & P  \left( ∑_{k=0}^N (t⋅ D)^k/ k!  \right)P^{-1}  \\
                       & =  & P ⋅
                            \begin{pmatrix}
                              \displaystyle ∑_{k=0}^N (-i ⋅t)^k/ k!   & 0 \\
                              0 &  \displaystyle ∑_{k=0}^N (i ⋅t)^k / k! 
                            \end{pmatrix} ⋅P^{-1}
  \end{eqnarray*}
  Puisque pour tout $t ∈ ℝ$ 
  \begin{displaymath}
    ∑_{k=0}^∞ (- i ⋅ t )^k/ k!  =  \cos(t) - i \sin(t)  \quad  \text{ et } \quad  ∑_{k=0}^∞ ( i ⋅ t )^k  / k! =   \cos(t) + i \sin(t),
  \end{displaymath}
et 
  \begin{displaymath}
    P^{-1}  = \frac{1}{2}
    \begin{pmatrix}
      1 & - i \\
      1 &  i
    \end{pmatrix} 
  \end{displaymath} on a   alors 
  \begin{eqnarray*} e^{t⋅A} & = &  
    P ⋅ D ⋅ P^{-1} \\
    & = &  \begin{pmatrix}
    \cos(t)&-\sin(t)\\
    \sin(t)&\cos(t)
  \end{pmatrix}    
  \end{eqnarray*}
  La solution respectant les conditions initiales $v =
  \begin{pmatrix}
    1 \\ 2
  \end{pmatrix}$ est, comme avant \begin{displaymath}
    \x =  \begin{pmatrix}
    \cos(t)&-\sin(t)\\
    \sin(t)&\cos(t)
  \end{pmatrix}     ⋅ \begin{pmatrix}
    1 \\ 2
  \end{pmatrix} =  \begin{pmatrix}
    \cos(t)-2\sin(t)\\
    2\cos(t)+\sin(t)
\end{pmatrix} . 
  \end{displaymath}
\end{example}


% \begin{example}
%   \label{exe:56}
%   Soit $A∈ ℝ^{4 ×4}$ la matrice
%   \begin{displaymath}
%     A = \left(\begin{array}{rrrr}
%                 5 & -2 & -1 & 5 \\
%                 4 & -1 & -1 & 3 \\
%                 -3 & 3 & 3 & -2 \\
%                 -1 & 1 & 0 & -1
%               \end{array}\right).
%           \end{displaymath}
% On va résoudre le problème
% \begin{displaymath}
%   \x' = Ax, \, \text{ sous conditions initiales }  \x(0) =
%   \begin{pmatrix}
%     \\3\\2\\1\\0
%   \end{pmatrix}
% \end{displaymath}
% La forme normal de Jordan $J$ de $A$ est
% \begin{displaymath}
%   J =  \left(\begin{array}{rrrr}
% - i & 0 & 0 & 0 \\
% 0 &  i & 0 & 0 \\
% 0 & 0 & 3 & 1 \\
% 0 & 0 & 0 & 3
%                        \end{array}\right) 
% \end{displaymath}
% et
% \begin{displaymath}
%   A = P^{-1}JP
% \end{displaymath}
% où
% \begin{displaymath}
%   P =  \left(\begin{array}{rrrr}
% 1 & 1 & 1 & -1 \\
%  i & - i & 1 & -1 \\
% - i &  i & 0 & -1 \\
% -1 & -1 & 0 & 0
% \end{array}\right). 
% \end{displaymath}
% Maintenant
% \begin{eqnarray*}
%   e^{tJ} & = & 
%   \begin{pmatrix}
%     \cos(t)  - i \sin(t) & 0& 0 & 0 \\
%    0 &    \cos(t) + i \sin(t)& 0 & 0 \\
%     0 & 0 & e^{3t} & 0 \\
%      0 & 0  & 0 & e^{3t} \\
%    \end{pmatrix} ⋅
%    \begin{pmatrix}
%      1 & 0 & 0 & 0 \\
%      0 & 1 & 0 & 0 \\
%      0 & 0 & 1 & t \\
%      0 & 0 & 0 & 1
%    \end{pmatrix} \\
%          & = &
%                 \begin{pmatrix}
%     \cos(t)  - i \sin(t) & 0& 0 & 0 \\
%    0 &    \cos(t) + i \sin(t)& 0 & 0 \\
%     0 & 0 & e^{3t} & 0 \\
%      0 & 0  & 0 & (t+1) e^{3t} \\
%    \end{pmatrix}
% \end{eqnarray*}
% Furthermore,
% \begin{displaymath}
% e^{tA} =   P^{-1} e^{tJ}  P = 
% \end{displaymath}
% \end{example}

\chapter{La forme normale de Jordan}
\label{cha:la-forme-normale}




\noindent 
Dans ce chapitre, on va démontrer le Théorème~\ref{thr:32}.  On va donner une forme normale spécifique pour chaque matrice complexe, le \emph{forme normale de Jordan}.
Soit $K$ un corps. 
 On se rappelle que une  matrice $N ∈ K^{n ×n}$ est \emph{nilpotente} s'il existe un $k \in \N$ tel que $N^k = 0$. 

% \smallskip
% \noindent 
% Le but est de montrer Théorème~\ref{thr:32}: 
% \begin{quote}  
%   Chaque matrice $A \in \C^{n \times n}$ peut être factorisée comme 
%   \begin{displaymath}
%     A = P ( \diag(\lambda_1,\dots,\lambda_n) + N) P^{-1}
%   \end{displaymath}
%   où $N \in \C^{n \times n}$ est nilpotente, $P \in \C^{n \times n}$ est inversible,  $\lambda_1,\dots,\lambda_n \in \C$ sont les valeurs propres de $A$ et $\diag(\lambda_1,\dots,\lambda_n)$ et $N$ commutent.
% \end{quote}




\begin{definition}
  Un \emph{bloc Jordan} est une matrice de la forme 
  \begin{displaymath} J(λ,k) = 
    \begin{pmatrix}
      λ & 1 \\
        & λ & 1 \\
        &   & \ddots & \ddots \\ 
        &   &             & λ & 1 \\
        &   &         &  & λ  \\
    \end{pmatrix} ∈ ℂ^{k ×k} 
  \end{displaymath}
où les éléments non décrits sont zéro. Le nombre $k$ est appelé la \emph{longueur} our la \emph{dimension} du bloc Jordan. 

Une matrice $A \in \C^{n \times n}$ est en \emph{forme normale de Jordan} si $A$ est en forme bloc diagonale, où tous les blocs sur la diagonale sont des blocs Jordan, i.e. $A$ est de la forme
\begin{displaymath}
  A =
  \begin{pmatrix}
    J(λ_1,n_1) \\
        & J(λ_2,n_2) \\
        &    & \ddots \\
        &    &       & J(λ_k,n_k)
  \end{pmatrix}
\end{displaymath}
où les matrices $J(λ_j,n_j) \in \C^{n_j\times n_j}$ sont des blocs de Jordan. 
\end{definition}


Notre but de ce chapitre  est de montrer le théorème suivant, qui est une version spécifique du  Théorème~\ref{thr:32}.

\begin{theorem}
  \label{thr:41}
  Soit $A \in \C^{n \times n}$, alors il existe des matrices $P,J \in \C^{n \times n}$ telles que $J$ est en forme normale de Jordan, $P$ est inversible et 
  \begin{displaymath}
    A = P^{-1} \,J \,P. 
  \end{displaymath}
\end{theorem}

\begin{definition}
  \label{def:36}
  Soit $V  = \C^n$. Le \emph{décalage}  est l'application linéaire 
  \begin{displaymath}
    U
    \begin{pmatrix}
      x_1 \\ \vdots \\ x_n
    \end{pmatrix}
     = 
     \begin{pmatrix}
       x_2 \\ x_3 \\ \vdots \\ 0
     \end{pmatrix}. 
  \end{displaymath}
  Le décalage plus une constante est aussi une application linéaire 
  \begin{displaymath}
    U + \lambda \cdot I. 
  \end{displaymath}
\end{definition}

Il est facile de voir que la matrice représentant le décalage plus $λ$ par rapport à la base canonique de $\mathbb{C}$ est 
un seul bloc de Jordan 
\begin{displaymath}
 \begin{pmatrix}
      λ & 1 \\
        & λ & 1 \\
        &   & \ddots & \ddots \\ 
        &   &             & λ & 1 \\
        &   &         &  & λ  \\
    \end{pmatrix}.   
\end{displaymath}




\section{Déterminer la forme normale der Jordan}
\label{sec:determiner-la-forme}

\noindent 
Avant de démontrer le Théorème~\ref{thr:41}, on se pose la question: \emph{Comment calculer une forme normale de Jordan d'une matrice $A ∈ ℂ^{n×n}$?}

On ce rappelle du fait suivant.
\begin{quote}
  Soit $K$ un corps. Si $A ,B ∈ K^{n ×n}$ sont similaires, alors leurs polynômes caractéristiques sont les mêmes. 
\end{quote}

\noindent 
Soit  alors le polynôme caractéristique de $A$, 
\begin{equation}
  \label{eq:57}
  p_A(x) = (-1)^n ∏_{i=1}^n (x - λ_{i}). 
\end{equation}
Alors $λ_1,\dots,λ_n$ sont les éléments sur la diagonale de $J$. 

\begin{example}
  \label{exe:52}

  Soit $A ∈ ℂ^{5 ×5}$ la matrice
  \begin{displaymath}
    A =  \left(\begin{array}{rrrrr}
-2 & 5 & -1 & 1 & 22 \\
8 & -7 & 0 & -4 & -32 \\
2 & -4 & 4 & 1 & -23 \\
-10 & 13 & -2 & 5 & 55 \\
-2 & 3 & 0 & 1 & 13
\end{array}\right)
\end{displaymath}
%
Le polynôme caractéristique de $A$ est
\begin{displaymath}
  p_A(x) = - (x-2)^4 (x-5) ∈ℂ[x]. 
\end{displaymath}
%
D'ici on sait alors que $J$ a la forme
\begin{displaymath}
  J =
  \begin{pmatrix}
    2 & *  \\
    & 2 & * \\
    & & 2 & * \\
    & & & 2   \\
    & & & & 5 \\
  \end{pmatrix}, 
\end{displaymath}
ou les $*$ peuvent être soit $0$ ou $1$. Si, par exemple, le deuxième $*$ est $0$ et les autres sont $1$, alors, $J$ est composé de $3$ blocs Jordan et totale. Si tous $*$ sont $1$, alors, $J$ est composé de $2$ blocs Jordan. 
\end{example}

Maintenant $A = P^{-1} J P$, où $P ∈ℂ^{n×n}$ est une matrice inversible.  Pour $λ ∈ ℂ$ on voit
\begin{displaymath}
  (A - λI)^k =  \left (P^{-1} (J - λ I) P \right)^k = P^{-1} (J - λ I)^k  P. 
\end{displaymath}
Un élément $x ∈ℂ^n$ est an élément du $\ker \left((A - λI)^k\right)$, si et seulement si $P x ∈ \ker \left((J - λI)^k\right)$. Alors on peut déduire le lemme suivant.
\begin{lemma}
  \label{lem:28}
  Soit $k ∈ ℕ$, alors
  \begin{displaymath}
     \dim(\ker \left((A - λI)^k \right)) = \dim(\ker \left((J- λI)^k \right)). 
  \end{displaymath}
\end{lemma}

\begin{example}[Exemple~\ref{exe:52} continué] 
  \label{exe:53}
  La dimension du $\ker(A - 2 I)$ est $2$ et $J - 2I$ est
  \begin{displaymath}
     J  - 2I =
  \begin{pmatrix}
    0 & *  \\
    & 0 & * \\
    & & 0 & * \\
    & & & 0   \\
    & & & & 3 \\
  \end{pmatrix}. 
\end{displaymath}
Alors, un des $*$ est zéro, les autres deux sont égaux à $1$. 
\end{example}

\begin{lemma}
  \label{lem:29}
  Soit $λ ∈ℂ$ une valeur propre de $A$, la dimension de
  \begin{displaymath}
    \ker( A - λI) 
  \end{displaymath}
  est le nombre de bloc Jordan de $J$ dont  $λ$ est sur  la diagonale.  
\end{lemma}


\begin{example}[Exemple~\ref{exe:52} continué] 
\label{exe:54}
  La dimension du $\ker(A - 2 I)$ est $2$ et $J - 2I$ est
  \begin{displaymath}
     J  - 2I =
  \begin{pmatrix}
    0 & *  \\
    & 0 & * \\
    & & 0 & * \\
    & & & 0   \\
    & & & & 3 \\
  \end{pmatrix}. 
\end{displaymath}
Alors, un des $*$ est zéro, les autres deux sont égaux à $1$.

On a deux blocs avec diagonale $2$. Les possibilités (à l'ordre pré) sont
\begin{displaymath}
J =   J_1 =  \begin{pmatrix}
    2 & 1  \\
    & 2 & 0 \\
    & & 2 & 1 \\
    & & & 2   \\
    & & & & 5 \\
  \end{pmatrix}
\end{displaymath}
et
\begin{displaymath}
J =   J_2 =  \begin{pmatrix}
    2 & 1  \\
    & 2 & 1 \\
    & & 2 & 0 \\
    & & & 2   \\
    & & & & 5 \\
  \end{pmatrix}
\end{displaymath}
Maintenant
\begin{displaymath}
  (J_1 - 2I)^2 =
  \begin{pmatrix}
     0 &0  \\
    & 0 & 0 \\
    & & 0 & 0 \\
    & & & 0   \\
    & & & & 9 \\
  \end{pmatrix}
\end{displaymath}
et
\begin{displaymath}
  (J_2 - 2I)^2 =
  \begin{pmatrix}
     0 &0 &1   \\
    & 0 & 0 \\
    & & 0 & 0 \\
    & & & 0   \\
    & & & & 9 \\
  \end{pmatrix}
\end{displaymath}
La dimension du noyaux  de $(A - 2I)^2$  est
\begin{displaymath}
\dim(  \ker((A - 2I)^2) ) = 3,
\end{displaymath}
alors
\begin{displaymath}
  J = J_2 =  \begin{pmatrix}
    2 & 1  \\
    & 2 & 1 \\
    & & 2 & 0 \\
    & & & 2   \\
    & & & & 5 \\
  \end{pmatrix}. 
\end{displaymath}
\end{example}


\begin{lemma}
  \label{lem:30}
  Soit $k ∈ ℕ$, $λ∈ ℂ$ une valeur propre de $A ∈ℂ^{n×n}$ et $J$ une forme normale de Jordan de $A$.   Le nombre de bloc Jordan dont $λ$ est l'élément diagonale et de longueur aux moins $k+1$   est donné par
  \begin{displaymath}
    \dim(\ker\left((A - λI)^{k+1}\right)) -\dim(\ker\left((A - λI)^{k}\right)). 
  \end{displaymath}
\end{lemma}

\begin{corollary}
  \label{cor:32x}
  Soit $A ∈ℂ^{n ×n}$. La forme normale de Jordan est unique a l'ordre des blocs pré. 
\end{corollary}




\section{Décomposition selon le polynôme caractéristique}
\label{sec:decomp-selon-le}


% \section{Polynômes}
% \label{sec:polyn-les-lalg}

% Soit $K$ un corps. 
% On dénote l'anneau des polynômes de $K$ par $K[x]$. 
% Un élément de $K[x]$ s'écrit comme 
% \begin{displaymath}
%   p(x) = a_0 + a_1 x + \cdots + a_n x^n 
% \end{displaymath}
% où les \emph{coefficients} $a_i \in K$. 

% La formule de multiplication de deux polynômes $f(x) = a_0+a_1x+ \cdots a_n x^n$ et $g(x) = b_0 + \cdots + b_m x^m$  est 
% \begin{equation}
% \label{eq:21}
%   f(x) \cdot g(x) = \sum_{i = 0}^{m+n} \left(\sum_{k+l = i}  a_{k} b_l\right) x^i
% \end{equation}


% \begin{definition}
%   \label{def:30}
%   Un polynôme $f(x) \in K[x]$
%   tel que $\deg(f) \geq 1$ est \emph{irréductible} si
%   \begin{displaymath}
%     f(x) = g(x) \cdot h(x) 
%   \end{displaymath}
% implique $\deg(g) \cdot \deg(h) = 0$, alors un des facteurs est une constante. 
% \end{definition}




% \begin{definition}
%   \label{def:33}
%   Un diviseur commun de $a(x) \in K[x]$
%   et $b(x) \in K[x]$
%   est un diviseur de $a(x)$
%   et $b(x)$.
%   Un diviseur commun le plus grand de $a(x)$
%   et $b(x)$
%   est un diviseur commun de $a(x)$
%   et $b(x)$
%   tel que tous les autres  diviseurs communs de $a(x)$ et $b(x)$ le divisent. On dénote les plus grands diviseurs communs de $a$ et $b$ par $\emph{pgdc}(a,b)$ (ou, en anglais, $\emph{gcd}(a,b)$, greatest common divisor).
% \end{definition}


% \begin{theorem}
%   \label{thr:36}
%   Soient $a(x),b(x)$ deux polynômes, tels que $\left\{ a,b \right\} \neq \left\{ 0 \right\}$. Un polynôme 
%   \begin{equation}
%     \label{eq:25}   
%     d(x) = g(x) a(x) + h(x) b(x) \neq 0
%   \end{equation}
%   de degré minimal, où $g,h \in K[x]$ , est un plus grand diviseur commun de $a$ et $b$. 
% \end{theorem}

% \begin{proof}
%   On montre qu'un tel $d(x)$ est un diviseur commun de $a$ et $b$ en procédant par l'absurde. Supposons que $d$ ne divise pas $a$. Alors il existe $q$ et $r$ tels que 
%   \begin{displaymath}
%     a = q\cdot d +r 
%   \end{displaymath}
% et $\deg(r) < \deg(d)$. Alors 
% \begin{displaymath}
%   r = a - q\cdot d = (1 - g\,q) a - h\,q\,b
% \end{displaymath}
% est un polynôme de la forme~\eqref{eq:25} avec un degré strictement plus petit que celui de $d$. 

% Il est clair que tous les diviseurs communs de $a$ et $b$ divisent $d$. 
% \end{proof}




% \begin{theorem}
%   \label{thr:39}
%   Soit $p(x)$ irréductible et supposons que $p(x) \mid f(x) \cdot  g(x)$, alors $p(x)\mid f(x)$ ou $p(x) \mid g(x)$. 
% \end{theorem}

% \begin{proof}
% Si $p(x)$ ne divise ni $f(x)$ ni $g(x)$ alors $1 = f(x) h_1(x) + p(x)h_2(x)$ et 
% $1 = g(x) h_3(x) + p(x) h_4(x)$ alors $\gcd(p(x), f(x)g(x))=1$. 
% \end{proof}


% \begin{theorem}
%   \label{thr:40}
%   Un polynôme  $f(x) \in K[x]$, $f(x) ≠ 0$  a une factorisation 
%   \begin{displaymath}
%     f(x) = a^* \prod_j p_j(x)
%   \end{displaymath}
%   où $a^* \in K$ et les $p_j(x)$ sont irréductibles avec coefficient dominant $1$. Cette factorisation est unique sauf pour des permutations des $p_j$. 
% \end{theorem}





\begin{definition}
  \label{def:34}
  Soient $V$ un espace vectoriel sur un corps $K$,
  $T: V \rightarrow V$ un endomorphisme et
  $f(x) = a_0+ \cdots + a_n x^n\in K[x]$. L'\emph{évaluation de $f$
    sur $T$} est l'endomorphisme $f(T): V \rightarrow V$
  \begin{displaymath}
    f(A) = a_n A^n + a_{n-1}A^{n-1}+ \cdots + a_1 A + a_0 \mathrm{id},
  \end{displaymath}
  où $A^n = \underbrace{A \circ A \circ \dots \circ A}_{n \text{ fois}}$ et $\circ$ dénote la composition de fonctions. 
\end{definition}


\begin{remark}
  \label{rem:8}
  Pour $f,g ∈ K[x]$ et  $T: V \rightarrow V$ un endomorphisme, on a
  \begin{enumerate}[i)]
  \item $(f ⋅ g) (T) =  f(T) \circ g(T)$ et
  \item $(f + g) (T) =  f(T) + g(T)$.
  \end{enumerate}
  L'argument est  comme la démonstration du Théorème~\ref{thr:51}, où $T$ joue le rôle de $x$ et $ \mathrm{id}$ de $x^0$. 
\end{remark}


\begin{definition}
  \label{def:35}
  Soient $T:V \rightarrow V$ un endomorphisme et $W \subseteq V$ un sous-espace de $V$. On dit que $W$ est \emph{invariant sous $A$} si $T(x) \in W$ pour tout $x \in W$. 
\end{definition}



\begin{lemma}
  \label{lem:20}
  Soient $f(x) ∈ K[x]$ et $T:V ⟶  V$ un endomorphisme,  alors $\ker(f(T))$ est invariant sous $T$. 
\end{lemma}


\begin{proof}
  Si $v ∈ \ker(f(T))$ on trouve que
  \begin{eqnarray*}
    f(T)(T(v))  & = &  (f(T) \circ T) v \\ 
    &=&  (T    \circ  f(T) ) (v)  \\
    &=& T (f(T)(v)) \\
    &=& 0.
  \end{eqnarray*}

  Alors, $T(v) ∈ \ker(f(T))$. 
\end{proof}

\todo[inline]{Remark on blcok diagonal matrix decomposition}. 

\begin{theorem}
  \label{thr:37}
  Soit $T: V \rightarrow V$ un endomorphisme et soit $f(x) = f_1(x) \cdot f_2(x)$ tel que
  \begin{enumerate}[i)]
  \item $\deg(f_1) \cdot \deg(f_2) \neq 0$,
  \item $\gcd(f_1,f_2) = 1$ 
  \end{enumerate}
  alors 
  $
      \ker(f(T)) = \ker(f_1(T)) \oplus \ker(f_2(T)) 
   $.   
\end{theorem}
\begin{proof}
  Dès que $\gcd(f_1,f_2)=1$ il existe $g_1(x),g_2(x)$ tels que 
  \begin{displaymath}
    1 = g_1(x) f_1(x) + g_2(x) f_2(x)
  \end{displaymath}
  et alors 
  \begin{equation}
    \label{eq:26}   
    g_1(T) \cdot f_1(T) +  g_2(T) f_2(T) = I. 
  \end{equation}
  Pour $v \in \ker(f(T))$, alors 
\begin{displaymath}
   g_1(T) \cdot f_1(T) \cdot v  + g_2(T) f_2(T) \cdot v  = v. 
\end{displaymath}
Mais $g_1(T) \cdot f_1(T) \cdot v \in \ker(f_2(T))$  dès que 
\begin{displaymath}
  f_2(T) \cdot g_1(T) \cdot f_1(T) \cdot v =   g_1(T) \cdot f_1(T) ⋅ f_2(T) \cdot v = g_1(T) f(T) v = 0
\end{displaymath}
et d'une manière similaire on voit que $g_2(T) f_2(T) \cdot v \in \ker(f_1(T))$. Il reste à démontrer que la somme est directe. 

Soit alors $v ∈  \ker(f_1(T))  ∩  \ker(f_2(T))$.  L'équation~\eqref{eq:26} montre 
\begin{displaymath}
  v =  g_1(T) \cdot f_1(T) \, v+  g_2(T) f_2(T) \, v = 0,
\end{displaymath}
qui démontre que la somme est directe. 
\end{proof}



% \begin{theorem}
% \label{thr:min-poly}
% Soient $V$ un espace vectoriel de dimension finie sur un corps $K$, et $A: \, V \rightarrow V$ un endomorphisme.
% Il y a un polyn{\^o}me $m_A(x) \in K[x]$ de degr{\'e} minimal tel que $m_A(A) = 0$ et le coefficient dominant de $m_A(x)$ est $1$. En plus,
% \begin{enumerate}
% \item $m_A(x)$ est unique,
% \item si $p(A) = 0$ pour un $p \in K[x]$, alors $m_A(x)$ divise $p$, et
% \item pour $\lambda \in K$, on a $m_A(\lambda) = 0$ si et seulement si $p_A(\lambda) = 0$, o{\`u} $p_A(x)$ est le polyn{\^o}me charact{\'e}ristique de $A$.
% \end{enumerate}
% \end{theorem}
% \begin{definition}
% On appelle $m_A(x)$ de Th{\'e}or{\`e}me~\ref{thr:min-poly} le \emph{polyn{\^o}me minimal de $A$}.
% \end{definition}
% \begin{proof}
% \underline{Existence:}
% Car $V$ est de dimension finie, l'espace vectoriel des endomorphismes $V \rightarrow V$ est de dimension finie.
% Alors, il existe un $k \in \N$ minimal tel que les endomorphismes
% \[
%  \mathrm{id}, A, A^2, \dots A^k
% \]
% sont lin{\'e}airement d{\'e}pendants,
% $
% 0 = \sum_{j=0}^k \alpha_j A^j$ pour quelques $\alpha_j \in K$, et $\alpha_k \neq 0$.
% On d{\'e}finit $m_A(x) = \sum_{j=0}^k (\alpha_j / \alpha_k) x^j$.
% Car on choisit $k$ minimal, $\{\mathrm{id},\dots,A^{k-1}\}$ est un ensemble libre et alors $m_A(x)$ est de degr{\'e} minimal.

% \underline{$i)$ et $ii)$:} Soit $p(x) \in K[x]$ polyn{\^o}me quelconque tel que $p(A) = 0$.
% Car il y a deux polyn{\^o}mes $g,h$ tel que 
% \[
% \gcd (m_A, p) (x) = g(x)m_A(x) + h(x) p(x) \neq 0,
% \]
% on a $\gcd (m_A, p) (A) = 0$.
% Car $m_A$ est de degr{\'e} minimal, on a $\deg (\gcd(m_A,p)) = \deg (m_A)$, et car le polyn{\^o}mes ont le coefficient dominant $1$, on a $m_A(x) = \gcd (m_A, p) (x)$, alors $ii)$ est vrai.
% De plus, si $\deg (p) = \deg (m_A)$, on a $p = \gcd (p, m_A)$ aussi, ce qui implique $i)$.

% \underline{$iii)$}
% Car $m_A \mid p_A$, si $m_A(\lambda) = 0$ alors $p_A(\lambda) = 0$.

% Si $p_A(\lambda) = 0$, il existe $v \in V$ tel que $Av = \lambda v$.
% Alors,
% \[
% 0 (v) = m_A(A) (v) = \left( \sum_{j=0}^k a_j A^j \right) (v) = \sum_{j=0}^k a_j A^j(v) = \left( \sum_{j=0}^k a_j \lambda^j \right) (v) = m_A (\lambda) v,
% \]
% et on a $iii)$.
% \end{proof}





\begin{lemma}
  \label{lem:19}
  Soit $V$ un espace vectoriel de dimension finie sur $\C$ et soit $T\colon V ⟶ V$ une application linéaire. Alors $V$ est la somme directe de sous-espaces   $V = V_1 ⊕ \cdots ⊕ V_K$  tels que 
  \begin{enumerate}[i)]
  \item $T(V_i) ⊆ V_i$ pour tout $i$ et \label{item:10}
  \item $T_{∣V_i} \colon V_i ⟶ V_i$
    est de la forme $N_i + λ \, I$ où $N_i$ est nilpotente. \label{item:11}
  \end{enumerate}
\end{lemma}

\begin{proof}
Soit $p(x) =$ le polyn{\^o}me caractéristique  de $T$, alors $p(T) = 0$.
Le coefficient dominant de $p(x)$ est $1$.   Le théorème fondamental de l'algèbre implique que 
\begin{displaymath}
  p(x) = ( x - λ_1)^{m_1} \cdots ( x - λ_k)^{m_k} 
\end{displaymath}
avec des $λ_i$ différents. 
Le diviseur le plus grand de $( x - λ_i)^{m_i}$ et $p(x) / ( x - λ_i)^{m_i}$ est $1$. En utilisant théorème~\ref{thr:37} en $k-1$ étapes, alors 
\begin{displaymath} 
V =   \ker p(T) = \ker (T - λ_1I)^{m_1} ⊕  \cdots ⊕ \ker( T - λ_kI)^{m_k}
\end{displaymath}
et avec $V_i = \ker( T - λ_iI)^{m_i}$ on a $V = V_1 ⊕ \cdots ⊕ V_K$  et \ref{item:10}) avec lemme~\ref{lem:20}. \newline

De plus, $$T_{∣V_i} = (T - λ_iI)_{∣V_i} + λ_iI_{∣V_i} =\colon N_i + λ_iI$$ et $N_i = (T - λ_iI)_{∣V_i}$ est bien nilpotente, car $V_i = \ker( T - λ_iI)^{m_i}$ et donc $N_i^{m_i} = (T - λ_iI)^{m_i}_{∣V_i} = 0$.
\end{proof}


\begin{remark}
  \label{rem:3}
  Lemme~\ref{lem:19} démontre que pour tout espace vectoriel $V$ sur $\mathbb{C}$ il existe une base 
  \begin{displaymath}
  \mathscr{B} =   b_1^1,\dots,b_{\ell_1}^1,b_1^2,\dots,b_{\ell_2}^2,\dots,b_1^k,\dots,b_{\ell_k}^k
  \end{displaymath}
  où $b_1^i,\dots,b_{\ell_i}^i$ est une base de $V_i$ telle que la matrice $A^T_\mathscr{B}$ de $T$ par rapport à la base $\mathscr{B}$ est une matrice bloc diagonale 
  \begin{displaymath}
      A^T_{\mathscr{B} }  =
      \begin{pmatrix}
        B_1 \\
        & B_2 \\
        & & \ddots \\
        & &  & B_k
      \end{pmatrix}
  \end{displaymath}
  et les matrices $B_i \in \C^{\ell_i × \ell_i}$ sont de la forme $B_i = N_i + λ_i I$ où les $N_i$ sont  nilpotentes.


  
Des que  les $N_i$ et $ λ_i I$ commutent, ça démontre le Théorème~\ref{thr:32}. 


\begin{example}
  \label{exe:47}
  Soit
  \begin{displaymath}
    A = \left(\begin{array}{rrr}
25 & 34 & 18 \\
-14 & -19 & -10 \\
-4 & -6 & -1
\end{array}\right)
\end{displaymath}

Le polynôme caractéristique de $A$ est $p_A(x) = -  (x-1)^2 (x-3)$. Alors
\begin{displaymath}
  \ker(p_A(A))  = ℝ^3 = \ker\left((A - I)^2\right) ⊕  \ker(A - 3I). 
\end{displaymath}
On a
\begin{displaymath}
(A - I)^2 =   \left(\begin{array}{rrr}
28 & 28 & 56 \\
-16 & -16 & -32 \\
-4 & -4 & -8
\end{array}\right)
\end{displaymath}
et une base du $\ker\left((A - I)^2\right)$ est
\begin{displaymath}
  \left\{
    \begin{pmatrix}
      2\\0\\-1
    \end{pmatrix}, \,
    \begin{pmatrix}
      0\\2\\ -1
    \end{pmatrix}
\right\} 
\end{displaymath}
Et
\begin{displaymath}
A - 3I =   \left(\begin{array}{rrr}
22 & 34 & 18 \\
-14 & -22 & -10 \\
-4 & -6 & -4
                 \end{array}\right)               
\end{displaymath}
avec base de noyaux
\begin{displaymath}
  \left\{
    \begin{pmatrix}
      -7 \\ 4 \\ 1
    \end{pmatrix} \right\} 
\end{displaymath}
Avec matrice
\begin{displaymath}
  P = \left(\begin{array}{rrr}
2 & 0 & -7 \\
0 & 2 & 4 \\
-1 & -1 & 1
\end{array}\right)
\end{displaymath}
on a
\begin{displaymath}
  P^{-1}AP = \left(\begin{array}{rrr}
16 & 25 & 0 \\
-9 & -14 & 0 \\
0 & 0 & 3
                   \end{array}\right)                 
\end{displaymath}
et
\begin{displaymath}
\left(\begin{array}{rr}
15 & 25 \\
-9 & -15
\end{array}\right)^2 = 0 
\end{displaymath}

Alors
\begin{displaymath} 
P^{-1}AP =
\begin{pmatrix}
  N_1 +  1 ⋅I_2 & 0 \\
  0         & N_2 + 3  ⋅ I_1
\end{pmatrix}
\end{displaymath}
est en forme blocque diagonale, où $N_1$ et $N_2$ sont nilpotentes. En fait, $N_2=0$. 
\end{example}

\subsection*{Exercices}
\label{sec:exercices}
\begin{enumerate}
\item Montrer que $K[x]$ est un anneau avec $1_{K[x]} = 1_K$.  
\item Montrer que  $a(x) \in K[x]$ et $b(x) \in K[x]$ $\deg(a)+\deg(b)>0$  possèdent exactement un diviseur commun le plus grand avec coefficient principal égal à $1_K$.  
\item Soit $V$ un espace vectoriel de dimension fini sur $ℂ$,  $T : V ⟶V$ un endomorphisme et $f(x) = (x - λ)^m ∈ ℂ[x]$. Montrer que $\ker(f(T)) \neq \{0\}$ si et seulement si $λ$ est un valeur propre de $T$. 
\end{enumerate}





%  Dès que l'espace des applications linéaires sur $V$ est un espace vectoriel sur $\C$ de dimension finie, il existe un $k \in \N$ tel que 
%  \begin{displaymath}
%    I, \, T, \, T^2, \dots, T^k
%  \end{displaymath}
%sont linéairement dépendants. De plus, il existe un polynôme $p(x) \in \C[x] \setminus \{ 0 \}$ tel que $p(T) = 0$. En effet, on peut choisir $p(x)$ le polynôme 
%caractéristique de $T$.

Rappel: Si  $\phi_{\mathscr{B}}$ est l'ismorphisme $\phi_{\mathscr{B}} \colon V \longrightarrow \C^n$, où $\phi_{\mathscr{B}}(x) = [x]_{\mathscr{B}}$ sont les coordonnées de $x$ par rapport à la base ${\mathscr{B}}$,  on a le diagramme suivant 
\begin{displaymath}
  {
  \begin{CD}
    V     @>T>>  V\\
    @VV \phi_{\mathscr{B}} V        @VV \phi_{\mathscr{B}} V\\ 
    \C^n     @>A^T_{\mathscr{B}} \cdot x>>  \C^n
  \end{CD}} 
\end{displaymath} 
\end{remark}

Il est clair, qu'il faut s'occuper maintenant des applications linéaires 
\begin{displaymath}
  T_{∣V_i} \colon V_i ⟶ V_i 
\end{displaymath}
qui sont de la forme $N + λ I$ pour une application nilpotente $N$. Le théorème suivant s'occupe des applications linéaires nilpotentes. La matrice de $λI$ est toujours $λ I$ pour chaque base. Il est alors clair que le théorème suivant démontre le théorème~\ref{thr:41}. 

\begin{theorem}
  \label{thr:38}
  Soit $V$ un espace vectoriel sur $\C$ de dimension finie et $N\colon V ⟶V$  une application linéaire nilpotente.  Alors $V$ possède une base $ℬ$ de la forme 
  \begin{displaymath}
    x_1,Nx_1, \dots, N^{m_1-1}x_1, x_2,Nx_2, \ldots , N^{m_2-1}x_2, \quad \dots \quad , x_k,Nx_k, \dots, N^{m_k-1}x_k
  \end{displaymath}
telle que $N^{m_i}x_i = 0$ pour tout $i$. 
\end{theorem}

\begin{remark}
  \label{rem:4}
  Si on inverse l'ordre de chaque orbite $x_i, Nx_i, \dots, N^{m_i-1} x_i$,
  %Si on inverse l'ordre de la base $ℬ$ et si on liste les éléments de droite à gauche
  on obtient une base $ℬ'$ et la matrice $A_{ℬ'}^{N }$ de l'application $N$  a la forme 
  \begin{displaymath}
    A_{ℬ'}^N =
    \begin{pmatrix}
      J_1 \\
      & J_2 \\
      & & \ddots \\
      & & & J_k
    \end{pmatrix}
  \end{displaymath}
en forme normale de Jordan, où 
\begin{displaymath}
  J_i =
  \begin{pmatrix}
    0 & 1 \\
    &  0 & 1 \\
    &    & \ddots & \ddots \\
    &    &        & 0 & 1 \\
    & & & & 0
  \end{pmatrix} \in \C^{m_i × m_i}. 
\end{displaymath}
Par conséquent, $N+ λI$ est représentée par 
\begin{displaymath}
 A_{ℬ'}^{N + λI} =    A_{ℬ'}^N + λ I_n
\end{displaymath}
en forme normale de Jordan. 
\end{remark}


\begin{proof}[Démonstration du Théorème~\ref{thr:38}] 
  Pour $x \in V \setminus \{0\}$ on appelle 
  \begin{displaymath}
    m_x = \min \{ i \colon N^ix = 0\}
  \end{displaymath}
  la \emph{durée de vie} de $x$. 
  La séquence 
  \begin{displaymath}
    x, Nx, \dots, N^{m_x-1} x
  \end{displaymath}
  est l'\emph{orbite} de $x$ sous $N$. 
  
  En concaténant les orbites des éléments d'une base et en travaillant sur cet ensemble, nous obtiendrons un ensemble de vecteurs qui engendrent $V$. 
  Supposons alors qu'au début de l'étape $q$, nous avons un ensemble $x_1,\dots,x_\ell$ avec $x_1,\dots,x_\ell \neq0$  dont les orbites 
  \begin{equation}
    \label{eq:27}
    x_1,Nx_1,\dots,N^{m_1-1}x_1, \,\dots \, ,  x_\ell,Nx_\ell,\dots,N^{m_\ell-1}x_\ell
  \end{equation}
  engendrent $V$ (pour la première étape, on prend $\ell = n$ avec des $x_i$ formant une base de $V$). Ici $m_i$ est la durée de vie de $x_i$. Si~\eqref{eq:27} est linéairement dépendant, nous allons soit supprimer un $x_i$ et son orbite (car superflus), soit remplacer un $x_i$ par un vecteur $y$ tel que 
  \begin{enumerate}[i)]
  \item Les orbites de $x_1,\dots, x_{i-1},y,x_{i+1},\dots,x_{\ell}$ engendrent aussi l'ensemble  $V$, 
  \item la somme des durées de vie de $x_1,\dots, x_{i-1},y,x_{i+1},\dots,x_{\ell}$  est strictement plus petite que la somme des durées de vie de $x_1,\dots,x_{\ell}$. 
  \end{enumerate}
Cela prouvera le théorème parce qu'un tel procédé doit se terminer. \\

Dès que l'ensemble~\eqref{eq:27} est linéairement dépendant, il existe une combinaison linéaire non triviale de~\eqref{eq:27} qui est égale a $0$ :
\begin{displaymath}
0 =   β_0^1 x_1 + β_1^1 Nx_1+ \dots+ β_{m_1-1}^1 N^{m_1-1}x_1 + \dots + 
β_0^\ell x_\ell + β_1^\ell Nx_\ell+ \dots+ β_{m_\ell-1}^\ell N^{m_\ell-1}x_\ell 
\end{displaymath}

\textbf{Cas 1 :} \\
Supposons que dans notre ensemble $x_1,Nx_1,\dots,N^{m_1-1}x_1, \,\dots \, ,  x_\ell,Nx_\ell,\dots,N^{m_\ell-1}x_\ell$, il existe $i$ tel que la durée de vie de $x_i$ est $1$ (i.e. $Nx_i = 0$ et l'orbite associée est seulement constituée de $x_i$) et supposons que ce $x_i$ apparaisse (avec un coefficient non nul) dans la combinaison linéaire ci-dessus. \\
En passant tous les termes sauf $x_i$ à gauche, on obtient  que $x_i$ est une combinaison linéaire non triviale des éléments de $\{ x_1,Nx_1,\dots,N^{m_1-1}x_1, \,\dots \, ,  x_\ell,Nx_\ell,\dots,N^{m_\ell-1}x_\ell \} \setminus \{x_i\}$. Donc on peut supprimer $x_i$ de cet ensemble et on obtient un nouvel ensemble de la même forme qu'en~\eqref{eq:27}, engendrant le même espace, mais avec une orbite en moins. \\

\textbf{Cas 2 :} \\
Supposons que nous avons la combinaison linéaire ci-dessus, mais que nous ne sommes pas dans le cas $1$. \\
Maintenant, nous allons appliquer l'application $N$ $k$-fois, où $k \geq 0$ est le plus grand entier tel que les termes 
\begin{displaymath}
  β_i^j N^{k+i}x_j 
\end{displaymath}
ne sont pas tous égaux à zéro. Ainsi, nous avons trouvé un sous-ensemble $J ⊆ \{1,\dots,\ell \}$ et des $γ_j ≠ 0$ tels que 
\begin{displaymath}
  \sum_{j \in J} γ_j N^{m_j-1}x_j = 0.
\end{displaymath}
Soit $m = \min_{j \in J} {m_j-1} \geq 1$ et soit $i \in J$ un index où le minimum est atteint. Alors 
\begin{displaymath}
 0 =  N^m  \sum_{j \in J} γ_j N^{m_j-1 - m}x_j  = N^m \left( γ_i x_i + \sum_{j \in J, j \neq i} γ_j N^{m_j-1 - m}x_j \right)
\end{displaymath}

Maintenant, en posant $$y = \sum_{j \in J} γ_j N^{m_j-1 - m}x_j = γ_i x_i + \sum_{j \in J, j \neq i} γ_j N^{m_j-1 - m}x_j$$ 
Si $y \neq 0$, on remplace $x_i$ par $y$.
Il est alors facile de voir que les orbites de 
\begin{displaymath}
  x_1,\dots, x_{i-1},y,x_{i+1},\dots,x_\ell
\end{displaymath}
engendrent encore $V$. Et la durée de vie de $y$ est au plus $m<m_i$. \\
Sinon, les orbites de
\begin{displaymath}
  x_1,\dots, x_{i-1},x_{i+1},\dots,x_\ell
\end{displaymath}
suffisent alors à engendrer $V$.

On a alors démontré le théorème.  
\end{proof}


\begin{example}
  \label{exe:57}

  On considère la matrice
  \begin{displaymath}
    N =  \left(\begin{array}{rrrr}
0 & -1 & -1 & 1 \\
-1 & 1 & 1 & -1 \\
1 & -1 & -1 & 1 \\
0 & -1 & -1 & 0
\end{array}\right).  
\end{displaymath}
On a
\begin{displaymath}
  N^3 = \left(\begin{array}{rrrr}
0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & -1 & -1 & 0 \\
0 & 0 & 0 & 0
\end{array}\right) 
\end{displaymath}
et
\begin{displaymath}
  N^4 = 0. 
\end{displaymath}
La durée de vie de la deuxième colonne de  $N$ est 4. 
\end{example}



\subsection*{Exercices} 

\begin{enumerate}
\item Montrer que les \textbf{orbites} de 
$ x_1,\dots, x_{i-1},y,x_{i+1},\dots,x_\ell $ engendrent encore $V$. (Voir démonstration du théorème~\ref{thr:38}). 

\item Le but de cet exercice est de faire la preuve du Théorème~\ref{thr:41} "à l'envers". \newline
Soit $T \colon V \rightarrow V$ un endomorphisme. Soit $\phi : V \rightarrow \Bbb C^n$ l'isomophisme associé à une base $B$ de $V$ et à la base canonique $E$ de $\Bbb C^n$. Supposons que $A_{B} = ([T(b_1)]_E, ..., [T(b_n)]_E)$, la matrice de $T$ relativement à la base $B$, admette une forme normale de Jordan $J$ avec matrice de passage $P = (p_1, ..., p_n)$. \newline
Montrer qu'il existe des sous-espaces $V_1, ..., V_k$  de $V$ tels que pour tout $i$ :
\begin{enumerate}
\item $V = V_1 \oplus \cdots \oplus V_k$;
\item $V_i = \phi(\text{span}(p_{k_i}, ..., p_{k_i + l_i}))$;
\item $T(V_i) \subset V_i$;
\item $T_{∣V_i} = N_i + \lambda_i I$, où $N_i \colon V_i \rightarrow V_i$ est nilpotente;
\item $\{\lambda_1, ..., \lambda_k\} = \{J_{11}, ..., J_{nn}\}$.
\end{enumerate} 

\item Le but de cet exercice est de montrer les propriétés des décompositions comme dans le Lemme~\ref{lem:19}. \newline
Soit $T \colon V \rightarrow V$ un endomorphisme et soit $V_1, ..., V_k$ une décomposition de $V$ tel que $V = V_1 ⊕ \cdots ⊕ V_k$, $T(V_i) \subset V_i$ et $T_{∣V_i} = N_i + \lambda_i I$, où $N_i : V_i \rightarrow V_i$ est nilpotente. Montrer que :
\begin{enumerate}
\item[a)] $V_i \subset \ker (T - \lambda_i I)^{a_i}$ pour un entier $a_i$ tel que $N_i^{a_i} = 0$.
\item[b)] Les $\lambda_1, ..., \lambda_k$ sont des valeurs propres (pas forcément distinctes) de $T$. (\textit{Indice} : Utiliser par exemple le premier point).
\item[c)] Le polynôme $f(x) = \prod_{i=1}^{k} (x - \lambda_i)^{a_i}$ annule $T$. (\textit{Indice} : Montrer que $f(T)v = 0$ pour tout $v \in V$ en utilisant la décomposition de $V$ et le premier point).
\item[d)] En déduire que l'ensemble $\{ \lambda_1, ..., \lambda_q \}$ contient toutes les valeurs propres de $T$ (\textit{Indice} : Si $v \neq 0$ est un vecteur propre de $T$ de valeur propres $\lambda$, exprimer $f(T)v$ en fonction de $f$, $\lambda$, et $v$). 
\item[e)] En déduire que les valeurs sur la diagonale de n'importe quelle forme normale de Jordan de $T$ constituent l'ensemble des valeurs propres de $T$. (\textit{Indice} : Utiliser l'exercice 2.)
\end{enumerate}

\item Comparer les polynômes caractéristiques de $J$ et $A$. En déduire que les éléments diagonaux de $J$ contiennent exactement l'ensemble des valeurs propres de $A$ et le nombre d'apparitions de chaque valeur propre sur la diagonale de $J$ est égale à la multiplicité algébrique de ladite valeur propre.

\item Soit $A \in \Bbb C^{n \times n}$ et soient $J$ une forme normale de Jordan de $A$, $P$ la matrice de passage associée ($A = PJP^{-1}$). \newline 
Le but de cet exercice est de montrer que le nombre de blocs de Jordan sur $J$ associé à une valeur propre $\lambda$ est exactement $\dim \ker(A - \lambda I)$.
\begin{enumerate}

\item[a)] Soit $S = \begin{pmatrix} S_1 & 0 \\ 0 & S_2 \end{pmatrix}$ une matrice blocs diagonale. Montrer que $$\text{rang}(S) = \text{rang}(S_1) + \text{rang}(S_2)$$ 
Généraliser pour $p$ blocs sur la diagonale. (\textit{Indice} : Considérer les lignes linéairement indépendantes de $S_1, S_2$).
\item[b)] Soit $B = U + \lambda I \in \Bbb C^{q \times q}$ un bloc de Jordan, où $U$ est l'application de décalage. Montrez que la seule valeur propre de $B$ est $\lambda$ et que l'espace propre associé est engendré par $e_1$. Déduisez $\dim \ker (B - \lambda I) = 1$ et $\dim \text{Im} (B - \lambda I) = q-1$.
\item[c)] Soient $B_1, ..., B_k$ l'ensemble des blocs de Jordan sur $J$ associé à une valeur propre $\lambda$. Déduire de a) et b) que $\dim \text{Im} (J - \lambda I) = n - k$.
\item[d)] En déduire que $\dim \ker (A - \lambda I) = k$ et que $\ker (A - \lambda I) = \text{span}(Pe_{i_1}, ..., Pe_{i_k})$, où les $i_j$ sont les indices des premières lignes/colonnes des $B_1, ..., B_k$ dans $J$.
\end{enumerate}

\item Déduire des exercices $4$ et $5$ que si $A$ est diagonalisable, la forme normale de Jordan $J$ de $A$ est diagonale.

\item Soit $A \in \Bbb C^{n \times n}$ une matrice diagonalisable. Montrer que $\ker(A - \lambda I) = \ker(A - \lambda I)^k$ pour toute valeur propre $\lambda$ de $A$ et pour tout $k > 0$. (\textit{Indice} : Diagonaliser d'abord $(A - \lambda I)$ et $(A - \lambda I)^k$ de manière simultanée).

\item Trouver deux matrices $A \in \Bbb C^{n \times n}$ et $B \in \Bbb C^{n \times n}$ qui ont le même polynôme caractéristique, mais qui ne sont pas similaires (\emph{Rappel} : $A$ et $B$ sont dites similaires s'il existe une matrice $P$ inversible telle que $B = P^{-1} A P$).

\item Soit $J \in \Bbb C^{n \times n}$ une matrice en forme normale de Jordan. Montrer que $J$ et $J^T$ sont similaires. En déduire que pour tout $A \in \Bbb C^{n \times n}$, les matrices $A$ et $A^T$ sont similaires.
\end{enumerate}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
