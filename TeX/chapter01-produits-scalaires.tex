\chapter{Formes bilinéaires}
\label{cha:prod-scal-et}


\noindent 
Dans le Chapitre~\ref{cha:valeurs-propres-et} nous avons vu le concept de similarité de deux matrices $A , B ∈K ^{n ×n}$. Les matrices $A$ et $B$ sont semblables, s'il existe une matrice  inversible $P ∈K^{n ×n}$ tel que
\begin{displaymath}
  A = P^{-1} ⋅B ⋅P. 
\end{displaymath}
Le Théorème~\ref{thr:45} explique, quand une matrice $A ∈ K^{n ×n}$ est semblable à une matrice diagonale. C'est la cas si et seulement si le polynôme caractéristique de $A$ décompose en facteurs linéaires et le multiplicités algébriques et géométriques   de ces racines sont les mêmes.

Dans ce chapitre on ce concentre sur une autre relation d'équivalence.  Deux matrices $A,B \in K^{n\times n}$ sont dites \emph{congruentes} s'il existe une matrice $P \in K^{n \times n}$ inversible telle que 
\begin{displaymath}
  A = P^T B P.
\end{displaymath}
Nous écrivons dans ce cas $A \cong B$ et on se demande quand est-ce que une matrice $A ∈ K^{n ×n}$ est congruente à une matrice diagonale. $A \cong \diag(a_1,\dots,a_n)$. Dans ce cas, on a certainement
\begin{displaymath}
  A^T = A, 
\end{displaymath}
c.à.d. que $A$ est une matrice \emph{symétrique}.

Considérons un exemple. Soit $A ∈ ℤ_3^{ n × n}$ la matrice  
\begin{displaymath}
  A =
  \begin{pmatrix}
    0 & 1 \\
    1 & 0 
  \end{pmatrix}.
\end{displaymath}
L'effet de la multiplication de $A$ avec
\begin{displaymath}
  P_1 =  \begin{pmatrix}
    1 & 0\\
    1 & 1 
  \end{pmatrix} 
\end{displaymath}
à droite est que la nouvelle première colonne de $A$ est la somme des deux colonnes. L'effet de la multiplication de $A$ avec $P^T$ à gauche est que la nouvelle première ligne de $A$ est la somme des deux lignes. Alors on obtient
\begin{displaymath}
  P_1^T A  P_1 =
  \begin{pmatrix}
    2 & 1 \\
    1 & 0 
  \end{pmatrix}.
\end{displaymath}
D'ici on peut additionner la première colonne de $A$ sur la deuxième et additionner la  première ligne de $A$ sur la deuxième et on obtient avec 
\begin{displaymath}
  P_2 =  \begin{pmatrix}
    1 & 1\\
    0 & 1 
  \end{pmatrix} 
\end{displaymath}
\begin{displaymath}
  (P_1 P_2)^T A P_1P_2 =  \begin{pmatrix}
    2 & 0 \\
    0 & 1 
  \end{pmatrix}.
\end{displaymath}
%
Maintenant, si  $A ∈ ℤ_2^{ n × n}$  est encore la  matrice  
\begin{displaymath}
  A =
  \begin{pmatrix}
    0 & 1 \\
    1 & 0 
  \end{pmatrix}, 
\end{displaymath}
on observe que 
\begin{displaymath}
  P_1^T A  P_1 =
  \begin{pmatrix}
    0 & 1 \\
    1 & 0 
  \end{pmatrix}.
\end{displaymath}
En fait, on peut montrer que, sur $ℤ_2$, la matrice $A$ n'est pas congruente à une matrice diagonale.

Dans ce chapitre, nous allons voir que toute matrice symétrique $A ∈ K^{ n ×n}$ est congruente à une matrice diagonale si $1_K + 1_K ≠ 0$. 

\section{Formes bililnéaires: Définition et propriétés de base}
\label{sec:formes-bililneaires}

\begin{definition}
\label{def:58}
  Soit $V$
  un espace vectoriel sur un corps $K$.
  Une \emph{forme bilinéaire} sur $V$
  est une correspondance qui à tout couple $(v,w)$
  d'éléments de $V$ 
  associe un scalaire, noté $\pscal{v,w} ∈ K$,
   satisfaisant aux deux propriétés suivantes:
  \begin{enumerate}[BL 1]
  \item Si $u,v$ et $w$ sont des éléments de $V$, et $α ∈ K$ est un scalaire,  \label{ps:2}
    \begin{displaymath}
      \pscal{u,v+w} = \pscal{u,v}+\pscal{u,w} \quad \text{ et } \pscal{u,α ⋅ w} = α ⋅\pscal{u,w}. 
    \end{displaymath}
  \item 
    Si $u,v$ et $w$ sont des éléments de $V$, et $α ∈ K$ est un scalaire,  \label{ps:3}
    \begin{displaymath}
      \pscal{v+w,u} = \pscal{v,u}+\pscal{w,u} \quad \text{ et } \pscal{α ⋅ u, w} = α ⋅\pscal{u,w}. 
    \end{displaymath}     
  \end{enumerate}
La  forme bilinéaire  est dite \emph{symétrique} si 
pour tout $v,w \in V$ 
\begin{displaymath}
    \pscal{v,w} = \pscal{w,v}. 
\end{displaymath}
\noindent
On dit que la forme bilinéaire est \emph{non dégénérée à gauche} (respectivement \emph{à droite}) si la condition suivante est vérifiée:
\begin{quote}
  Si $v \in V$, et si $\pscal{v,w}=0$ pour tout $w \in V$, alors $v = 0$. 
\end{quote}
Si la forme bilinéaire est non dégénérée à gauche et à droite, on dit qu'elle est \emph{non dégénérée}.
\end{definition}

\begin{example}
 \label{ex:1}
  Soit $V = K^n$, l'application  
  \begin{displaymath}
    \begin{array}{rcl}
      \pscal{\,}\colon    \,  V\times V& \longrightarrow &K \\
                              (u,v)   & \longmapsto & \sum_{i=1}^n u_i v_i
    \end{array}  
  \end{displaymath}
est une forme bilinéaire. 
Vérifions (BL~\ref{ps:2}). Pour tous  $u,v,w ∈ K^n$ et $α ∈ K$:  
\begin{eqnarray*}
  \pscal{u,v+w} & = & \sum_{i=1}^n u_i \, (v_i+w_i) \\
  & = &  \sum_{i=1}^n \left( u_i v_i + u_i w_i \right) \\
  & = &  \sum_{i=1}^n  u_iv_i +\sum_{i=1}^n u_iw_i \\
  &= &\pscal{u,v} + \pscal{u,w}
\end{eqnarray*}
 et 
 \begin{displaymath}
   \pscal{u,α ⋅w} = \sum_{i=1}^n u_i α w_i = α ∑_{i=1}^n u_i w_i = α \pscal{u,w}. 
 \end{displaymath}
On appelle cette forme bilinéaire la \emph{forme bilinéaire standard} de $K^n$.   On vérifie aussi très facilement que la forme bilinéaire standard est symétrique et non dégénérée.
\end{example}



\begin{example}
  \label{exe:31}
  Soit $V = ℝ^3$ et
  \begin{displaymath}
    \pscal{u,v} = u^T
    \begin{pmatrix}
      1 & 0 & 1\\
      2 & 0 & 2 \\
      1 & 1 & 0\\
    \end{pmatrix}
    v \text{ pour } u,v ∈ ℝ^3,    
  \end{displaymath}
  est une forme bilinéaire non-symmetrique, dégénérée à droite  et à gauche. 
\end{example}


\begin{example}
  \label{ex:2}
  Soit $V$ l'espace des fonctions continues à valeurs réelles, définies sur l'intervalle $[0,2 \cdot \pi]$. Si $f,g \in V$ on pose 
  \begin{displaymath}
    \pscal{f,g} = \int_0^{2\pi} f(x)g(x) \, dx.
  \end{displaymath}
Clairement, $\pscal{,}$ est une forme bilinéaire symétrique sur $V$ non dégénérée. 
\end{example}

\begin{exercise}
  \label{ex:3}
  Montrer que les  formes bilinéaires  des exemples~\ref{ex:1} et \ref{ex:2} sont non dégénérées. 
\end{exercise}



Soit $V$ un espace vectoriel de dimension finie et $B = \{v_1,\dots,v_n\}$ une base de $V$. Pour une forme bilinéaire $f: V × V ⟶ K$ 
 et $x = \sum_i\alpha_i v_i$ et $y = \sum_j \beta_j v_j$ on a 
 \begin{eqnarray*}
   f(x,y) & = & f \left( ∑_{i=1}^n \alpha_i v_i ,   ∑_{j=1}^n \beta_j v_j \right) \\
          & = & ∑_{i=1}^n \alpha_i f \left(  v_i ,   ∑_{j=1}^n \beta_j v_j \right) \\
          & = & ∑_{i,j=1}^n \alpha_i β_j f (  v_i ,  v_j )
    \end{eqnarray*}
alors pour la matrice $A_B^f \in K^{n\times n}$, ayant comme composantes $f(v_i,v_j)$, on a 
\begin{displaymath}
  f(x,y) = [x]_B^T \,A_B^f \,  [y]_B. 
\end{displaymath}
\begin{exercise} Soit $V$ de dimension finie et $B$ une base de $V$. 
  Deux formes bilinéaires $f,g : V × V ⟶ K$
  sont différentes si et seulement si $A_B^f \neq A_B^g$. 
\end{exercise}


\begin{example}
  \label{exe:9}
  Soit $V = \{ p(x) : p ∈ ℝ[x], \, \deg(p)≤ 2\}$ l'espace vectoriel des polynômes réelles de degré au plus $2$ et $B = \{ 1,x,x^2\}$ une base de $V$ et $f(p,q) = \int_0^1 p(x) ⋅ q(x) dx$. Il est facile de vérifier que $f$ est une forme bilinéaire sur $V$. La matrice $A_B^f$ est 
  \begin{displaymath}
    A_B^f =
    \begin{pmatrix}
      1 & 1/2 & 1/3 \\
      1/2 & 1/3 & 1/4\\
      1/3 & 1/4 & 1/5
    \end{pmatrix}. 
  \end{displaymath}
 Pour $p(x) = 2+3\,x - 5\,x^2$ et $q(x) = 2\,x + 3\,x^2$ on obtient 
 \begin{displaymath}
   \int_0^1 f(x) p(x) dx =
   \begin{pmatrix}
     2 & 3 & -5
   \end{pmatrix}
 \begin{pmatrix}
      1 & 1/2 & 1/3 \\
      1/2 & 1/3 & 1/4\\
      1/3 & 1/4 & 1/5
    \end{pmatrix}
   \begin{pmatrix}
     0\\2\\3
   \end{pmatrix}
 \end{displaymath}
\end{example}


Pour mémoire, pour deux bases $B,B'$ et étant donné $[x]_{B'}$,  on trouve les coordonnées de $x$ dans la base $B$,  $[x]_B$,  à l'aide de la matrice de changement de base $P_{B'B}$ comme 
\begin{displaymath}
  [x]_B = P_{B'B} [x]_{B'}. 
\end{displaymath}
Cette formule nous montre que 
\begin{equation}
\label{eq:28}
 A_{B'}^f =  P_{B'B}^T \,A_B^f \,  P_{B'B}. 
\end{equation}

\begin{exercise}
  Soit $V$ un $K$-espace vectoriel de dimension finie et $B$ une base de $V$. 
  Une forme bilinéaire $f : V ×V ⟶ K$ 
  est symétrique si et seulement si $A_B^f$ est symétrique.
\end{exercise}

\begin{proposition}
  \label{prop:7}
  Soit $V$ un $K$-espace vectoriel de dimension finie, $B= \{b_1, \dots , b_n\}$ une base de $V$ et $f : V × V ⟶K$ une forme bilinéaire.   Les conditions suivantes sont équivalentes. 
  \begin{enumerate}[i)]
  \item $\rank(A_B^f) = n$ \label{item:12}
  \item $f$ est non dégénérée à gauche, i.e. si $v \in V$, et si $\pscal{v,w}=0$ pour tout $w \in V$, alors $v = 0$ \label{item:13}
  \item $f$ est non dégénérée à droite, i.e. si $v \in V$, et si $\pscal{w, v}=0$ pour tout $w \in V$, alors $v = 0$ \label{item:14}
  \end{enumerate}
\end{proposition}

\begin{proof} 

Nous montrons \ref{item:12}) et \ref{item:13}) sont équivalentes.  De la même manière, on démontre aussi que \ref{item:12}) et \ref{item:14}) sont équivalentes.

\ref{item:12}) $\Rightarrow$ \ref{item:13}) : Supposons que $\rank(A_B^f) = n$ et soit $v \in V$, $v \neq 0$. Pour $w \in V$ on a 
\begin{displaymath}
  f(v,w) = [v]_B^T A_B^f [w]_B. 
\end{displaymath}
Dès que $[v]_B \neq 0$, on a $[v]_B^T A_B^f \neq 0^T$ (car $\noy(A_B^f) = \{0\} \iff \rank(A_B^f) = n$). Supposons que la $i$-ème composante de $[v]_B^T A_B^f$ n'est pas égale a $0$. Alors $[v]_B^T A_B^f e_i \neq 0$ où toutes les composantes de $e_i$ sont $0$ sauf la $i$-ème composante, 
qui est égale a $1$. Alors $f(v,b_i) \neq 0$. Donc $f$ est non dégénérée à gauche. \newline

\ref{item:13}) $\Rightarrow$ \ref{item:12}) : Si $f$ est non dégénérée à gauche, alors $x^T A_B^f \neq 0$ pour tout $x \in K^n$ tel que $x \neq 0$ (sinon, on aurait trouvé un $x$ tel que $x^T A_B^f y = 0$ pour tout $y \in K^n$). Ceci implique que les lignes de $A_B^f$ sont linéairement indépendantes. Alors $\rank(A_B^f) = n$. 
\end{proof}






\section{Orthogonalité} 
\label{sec:orthogonalite}

\begin{framed}\noindent 
  Pour ce paragraphe~\ref{sec:orthogonalite}, s'il n'est pas spécifié autrement,  $V$
  est toujours un espace vectoriel sur $K$
  muni d'une forme bilinéaire symétrique $\pscal{,}$. 
\end{framed}



\begin{definition}
  \label{def:2}
 Deux éléments $u,v \in V$ sont \emph{orthogonaux} ou \emph{perpendiculaires} si $\pscal{u,v} = 0$, et l'on écrit $u \perp v$. 
\end{definition}

\begin{proposition}
  \label{prop:1}
  Soit $E \subseteq V$ une partie de $V$, alors $E^\perp = \{ v \in V \colon v \perp e \text{ pour tout } e \in E \}$   est un sous-espace vectoriel de $V$. 
\end{proposition}

\begin{proof}
  Pour mémoire: $\emptyset \neq W\subseteq V$ est un sous-espace si les conditions suivantes sont vérifiées. 
  \begin{enumerate}[i)]
  \item Si $u,v \in W$ on a $u+v \in W$.  
  \item Si $c \in K$ et $u \in W$ on a $c \cdot u \in W$. 
  \end{enumerate}
  
  \noindent
  Des que $0 ∈ E^\perp  $, on a que $E^\perp≠ ∅$.  %ici E^\perp \neq \varnothing peur résoudre le problème a la compilation : l'ensemble vide est affiché deux fois ...
  Si $u,v \in E^\perp$ alors pour tout $e \in E$ 
  \begin{displaymath}
%    \pscal{e,u+v} = \pscal{e,u} + \pscal{e,-v} = 0 + 0 = 0,
    \pscal{e,u+v} = \pscal{e,u} + \pscal{e,v} = 0 + 0 = 0,
  \end{displaymath}  
  et pour $c \in K$ 
  \begin{displaymath}
    \pscal{e,c\cdot v} = c \,\pscal{e,v} = c \cdot 0 = 0. 
  \end{displaymath}
\end{proof}



\begin{exercise}
  \label{exe:1}
  Soit $E \subseteq V$ et $E^*$ le sous-espace de $V$ engendré par les éléments de $E$. Montrer $E^\perp = {E^*}^\perp$. 
\end{exercise}


\begin{example}
  \label{exe:2}
  Soient $K$ un corps et $(a_{ij}) \in K^{m\times n}$ une matrice à $m$ lignes et $n$ colonnes. Le système homogène linéaire 
  \begin{equation}
    \label{eq:1}
    A\,X = 0,
  \end{equation}
  peut s'écrire sous la forme 
  \begin{displaymath}
    \pscal{A_1,X}=0, \dots ,\pscal{A_m,X}=0, 
  \end{displaymath}
  où les $A_i$  sont les vecteurs lignes de la matrice $A$ et $\pscal{,}$ dénote la forme bilinéaire standard de $K^n$. Soit $W$ le sous-espace de $K^n$ engendré par les $A_i$ et $U$ le sous-espace de $K^n$ des solutions du système~\eqref{eq:1}. Alors on a  $U = W^\perp$ et $\dim(W^\perp) = \dim(U) = n - \rank(A) = \dim(\noy(A))$. 
  
 \end{example}




\begin{definition}
  \label{def:8}
  La caractéristique d'un anneau (unitaire) $R$, $\mathrm{Char}(R)$ 
  est l'ordre de $1_R$
  comme élément du groupe abélien $(R,+)$.
  En d'autres mots, c'est le nombre 
  \begin{displaymath}
    \min_{k \in \N_+} \underbrace{1+ \cdots + 1}_{k \text{ fois }} = 0
  \end{displaymath}
  Si cet ordre est infini, la caractéristique de $R$ est $0$.
\end{definition}

\begin{notation}
  Pour $n \in \N_+$ l'anneau des classes des restes est dénoté comme
  $\Z / n \Z$ ou plus brièvement $\Z_n$ (parfois aussi noté $\Bbb F_n$). Ceci est un corps si et seulement si $n$ est un nombre premier. 
\end{notation}

\begin{example}
\label{exe:10}
   Soit $n \in \N_+$. Alors la  caractéristique de $\Z_n$ est $n$. 
   La caractéristique de $\Q,\R$ et $\C$ est zéro.    
\end{example}

\begin{lemma}
  \label{lem:1}
 Soit $\mathrm{Char}(K)\neq 2$. Si $\pscal{u,u}=0$ pour tout $u \in V$ alors 
  \begin{displaymath}
    \pscal{u,v}=0 \text{ pour tous }u,v \in V
  \end{displaymath}
On dit que la forme bilinéaire symétrique $\pscal{,}$ est  \emph{nulle}. 
\end{lemma}

\begin{proof}
  Soient $u,v \in V$. 
  On peut écrire
  \begin{displaymath}
   2 \cdot  \pscal{u,v} = \pscal{u+v,u+v} - \pscal{u,u} - \pscal{v,v} 
  \end{displaymath}

et comme $2 \neq 0$ on a $\pscal{u,v} = 0$. 
\end{proof}


\begin{definition}
  \label{def:38}
  Une base $\{v_1,\dots,v_n\}$
  de l'espace vectoriel $V$
  est une \emph{base orthogonale} si $\pscal{v_i,v_j}=0$
  pour $i\neq j$.
\end{definition}



\begin{remark}
  \label{rem:7}
  Pour une forme bilinéaire symétrique $\langle , \rangle$ et une base $B = \{v_1,\dots,v_n\}$. On se rappelle que 
  \begin{displaymath}
    〈v_i,v_j〉 = (A_B^{〈, 〉})_{ij}
  \end{displaymath}
  Alors $B$ est une base orthogonale, si et seulement si, $A_B^{\langle , \rangle}$ est une matrice diagonale.  
\end{remark}


\begin{theorem}
  \label{thr:5}
  Soit $\mathrm{Char}(K)\neq 2$
  et supposons que $V$
  est de dimension finie. Alors $V$ possède une base orthogonale.
\end{theorem}


\begin{proof}
  On montre le théorème par induction. Si $\dim(V) = 1$ alors toute base contient seulement un élément et alors est orthogonale. 

Soit $\dim(V) >1$.  Si $\pscal{u,u} = 0$ pour tout $u$, le lemme~\ref{lem:1} implique que la forme bilinéaire symétrique est nulle et toute base de $V$  est orthogonale.  
Autrement, soit $u \in V$ tel que $\pscal{u,u} \neq 0$ et soit $V_1 = \spa\{u\}$. Pour  $x \in V$ le vecteur 
\begin{displaymath}
  x - \pscal{x,u}/\pscal{u,u} \cdot  u \in V_1^\perp
\end{displaymath}
et alors $V = V_1 + V_1^\perp$. Cette somme est directe parce que chaque élément de $V_1 \cap V_1^\perp$ s'écrit comme $\beta \cdot u$ pour $\beta \in K$. Et $\pscal{u,\beta u} = \beta \pscal{u,u} = 0$ implique $\beta = 0$. 

Alors $\dim(V_1^\perp) < \dim(V)$, et par induction, $V_1^\perp$ possède une base orthogonale $\{v_2,\dots,v_n\}$. Alors $\{u,v_2,\dots,v_n\}$ est une base orthogonale de $V$. 
\end{proof}  



\begin{example}
  \label{exe:30}
  Soit $V = ℤ_5^3$ et $〈,〉： ℤ_5^3 × ℤ_5^3 → ℤ_5$ défini comme
  \begin{displaymath}
    〈x,y〉 = x^T A y,
  \end{displaymath}
  où
  \begin{displaymath}
    A = \left(\begin{array}{rrr}
                0 & 2 & 1 \\
                2 & 0 & 4 \\
                1 & 4 & 0
              \end{array}\right)
  \end{displaymath}
  Le but est de trouver une base orthogonale de $ℤ_5^3$. On va trouver une matrice inversible $P ∈ ℤ_5^{3×3}$ tel que $P^T A P$  est une matrice diagonale. Si  $p_1,p_2,p_3$ sont les colonnes de $P$, alors
  \begin{displaymath}
    \{ p_1,p_2,p_3 \}
  \end{displaymath}
  est une base de $ℤ_5^3$ et c'est une base orthogonale, des que
  \begin{displaymath}
    〈p_i,p_j〉 =0 \text{ si } i ≠j, \, 1 ≤i,j ≤3. 
  \end{displaymath}
  Nous allons additionner la $2$-ème colonne de $A$  sur la $1$-ère colonne et la $2$-ème ligne  de $A$  sur la $1$-ère ligne  de $A$. C'est à dire on calcule
  \begin{displaymath}
    P^T A P  = \left(\begin{array}{rrr}
4 & 2 & 0 \\
2 & 0 & 4 \\
0 & 4 & 0
\end{array}\right)
  \end{displaymath}
  où
  \begin{displaymath}
P  =  \left(\begin{array}{rrr}
1 & 0 & 0 \\
1 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)
  \end{displaymath}
  Après on va additionner $2⋅$ la $1$-ère colonne sur la deuxième, et l'opération correspondante de lignes et on obtient 
  \begin{displaymath}
    P^T A P  =
    \left(\begin{array}{rrr}
            4 & 0 & 0 \\
            0 & 4 & 4 \\
            0 & 4 & 0
          \end{array}\right)
      \end{displaymath}
      où
\begin{displaymath}
P = \left(\begin{array}{rrr}
1 & 2 & 0 \\
1 & 3 & 0 \\
0 & 0 & 1
\end{array}\right)        
\end{displaymath}
 Après on va additionner $4⋅$ la $2$-ère colonne sur la $3$-ème, et l'opération correspondante de lignes et on obtient 
  \begin{displaymath}
    P^T A P  =\left(\begin{array}{rrr}
4 & 0 & 0 \\
0 & 4 & 0 \\
0 & 0 & 1
\end{array}\right)    
  \end{displaymath}
      où
\begin{displaymath}
  P =\left(\begin{array}{rrr}
1 & 2 & 3 \\
1 & 3 & 2 \\
0 & 0 & 1
\end{array}\right).   
\end{displaymath}
Nous avons trouvé une base orthogonale
\begin{displaymath}
  \left\{
    \begin{pmatrix}
      1 \\
1  \\
0 
    \end{pmatrix},
    \begin{pmatrix}
      2 \\
 3  \\
 0 
    \end{pmatrix},
    \begin{pmatrix}
      3 \\
      2 \\
      1
    \end{pmatrix}
    \right\}.  
\end{displaymath}      
\end{example}

\subsection*{Exercices} 

\begin{enumerate}
\item Soit $K$ un corps. Si la caractéristique de $K$ est différente de zéro, alors elle est un nombre premier. 
\item Soit $K$ un corps fini. Montrer que $|K| = q^\ell$ pour un nombre premier $q$ et un nombre naturel $\ell ∈ \N$. \emph{Indication: $K$ est un espace vectoriel de dimension finie sur $\Z_q$ pour un $q$ premier.}
\item On considère les vecteurs  \label{item:1}
  \begin{displaymath}
    v_1 =
    \begin{pmatrix}
      1\\1\\0\\0
    \end{pmatrix}, 
 v_2 =
    \begin{pmatrix}
      0\\1\\1\\0
    \end{pmatrix}, 
\text{ et }
 v_3 =
    \begin{pmatrix}
      0\\0\\1\\1
    \end{pmatrix} \in \Z_2^4. 
  \end{displaymath}
Est-ce que $\spa\{v_1,v_2,v_3\}$ possède une base orthogonale par rapport à la forme bilinéaire symétrique standard de l'exemple~\ref{ex:1}? 

\item En considérant le forme bilinéaire symétrique standard de l'exemple~\ref{ex:1}, trouver une base orthogonale du sous-espace de $\Z_3^4$ engendré par 
  \begin{displaymath}
    v_1 =
    \begin{pmatrix}
      1\\1\\1\\0
    \end{pmatrix}, 
 v_2 =
    \begin{pmatrix}
      0\\1\\1\\1
    \end{pmatrix}, 
\text{ et }
 v_3 =
    \begin{pmatrix}
      1\\0\\1\\1
    \end{pmatrix} \in \Z_3^4. 
  \end{displaymath}



\end{enumerate}

\section{Matrices congruentes} 
\label{sec:class-des-matr}

% \begin{framed}\noindent 
%   Pour ce paragraphe~\ref{sec:class-des-matr}, s'il n'est pas spécifié autrement,  $V$
%   est toujours un espace vectoriel sur $K$
%   muni d'une forme bilinéaire symétrique $\pscal{,}$. 
% \end{framed}


\begin{definition}
  \label{def:13}
  Deux matrices $A,B \in K^{n\times n}$ sont dites \emph{congruentes} s'il existe une matrice $P \in K^{n \times n}$ inversible telle que 
\begin{displaymath}
  A = P^T B P.
\end{displaymath}
Nous écrivons dans ce cas $A \cong B$. 
\end{definition}


\begin{example}
  \label{exe:22}
  Si $V$ est de dimension finie et $B,B'$ sont deux bases de V, la relation \eqref{eq:28} montre que $A_B^{\pscal{,}} \cong A_{B'}^{\pscal{,}}$. 
\end{example}

\begin{lemma}
  \label{lem:6}
  La relation $\cong$ est une relation d'équivalence. 
\end{lemma}

\begin{proof}
  Voir exercice. 
\end{proof}


Le relation entre $\cong$ et le concept de l'orthogonalité   est précisée dans le lemme suivant. 
\begin{lemma}
\label{thr:13}
  Soit $V$ un espace vectoriel de dimension finie et $B = \{v_1,\dots,v_n\}$ une base quelconque. Alors $V$ possède une base orthogonale si et seulement s'il existe une matrice diagonale $D$ 
telle que 
 $A_B^{\pscal{.}} \cong D$. 
\end{lemma}
\begin{proof}
Si $B'$ est une base orthogonale de $V$, alors $A_{B'}^{〈,〉}$ est une matrice diagonale. Grâce à la relation \eqref{eq:28}, $A_{B}^{〈,〉}$ est congruente à une matrice diagonale. 


Si $A_B^{\pscal{.}} \cong D$ où $D ∈ K^{n×n}$ est une matrice diagonale, alors 
il existe une matrice $P ∈K^{n ×n}$ inversible, telle que 
\begin{displaymath}
  P^T A_B^{\pscal{.}} P = D. 
\end{displaymath}
La base  $B' = \{w_1,\dots,w_n\}$ donnée par les colonnes de $P$ (en tant que coordonées dans la base $B$) :
  \begin{displaymath}
     [w_j]_B =
  \begin{pmatrix}
    p_{1j}\\ \vdots \\ p_{nj} 
  \end{pmatrix} \iff w_j = \sum_{i=1}^n p_{ij} v_i, \quad \forall j=1, \dots, n,
  \end{displaymath}
 est donc une base orthogonale. 
\end{proof}




 

\begin{corollary}
  \label{co:4}
  Soit $K$ un corps de caractéristique différente de $2$. Toute matrice symétrique  $A \in K^{n \times n}$ est congruente à une matrice diagonale. 
\end{corollary}
\begin{proof}
Ceci est un corollaire du  lemme~\ref{thr:5} et du théorème~\ref{thr:13} 
parce que  $K^n$ muni de la forme bilinéaire symétrique $\pscal{u,v} = u^TAv$ possède une base orthogonale. 
\end{proof}

%The example below is uncomplete and thus makes no sense. Since I didn't know what was its purpose, I couldn't complete it.
%\begin{example}
%  \label{exe:14}
%  Soient $V$ un espace vectoriel sur $\R$ et $\{v_1,v_2,v_3\}$ une base de V. 
%\\ \todo[inline]{Finish or delete the preceding example}
%\end{example}





\section{Un algorithme }
\label{sec:un-algorithme}

Maintenant, nous allons formaliser la procédé appliquée dans example~\ref{exe:30}. 
\begin{algorithm}
  \label{alg:1}
  Cet algorithme trouve une matrice diagonale congruente à la matrice symétrique $A \in K^{n \times n}$ où $K$ est un corps tel que $\car(K) \neq 2$.  L'algorithme procède en $n$ itérations. Après la $(i-1)$-ème itération, $i \geq 1$, (aussi après  la $0$-ème itération) l'algorithme a transformé $A$ en une matrice congruente 
  \begin{equation}
    \label{eq:7}
    \begin{pmatrix}
      c_1 \\
      & c_2 \\
      & & \ddots & &&\\
      & & & c_{i-1} \\
      & & & &  b_{i,i} & \dots & b_{i,n} \\
%      & & & &  b_{i+1,i} & \dots & b_{i+1,n} \\
      & & & &     \vdots       &  & \vdots \\
      & & & &  b_{n,i} & \dots & b_{n,n} \\      
    \end{pmatrix}
  \end{equation}
où les composantes des premières $(i-1)$ lignes et colonnes sont nulles sauf éventuellement sur la diagonale. 

\medskip 
\noindent 
Pour $1 \leq i \leq n$, la \emph{$i$-ème itération} procède comme suit. 
\begin{itemize}
 \item Soit l'indice $k$ minimal tel que $k \geq i$ et $b_{kk} \neq 0$. On échange la $i$-ème ligne et la $k$-ème ligne puis la $i$-ème colonne et la $k$-ème colonne. Ceci permet (entre autres) d'échanger les coefficients $b_{ii}$ et $b_{kk}$ de la diagonale, s'assurant ainsi d'avoir un coefficient non nul. 
 
 \item Si l'indice $k$ de l'étape précédente n'existe pas (tous les coefficients diagonaux après $c_{i-1}$ sont nuls), soit $j \in \{i+1,\dots,n\}$ un indice vérifiant $b_{ij} \neq 0$. On ajoute la $j$-ème ligne à la $i$-ème ligne puis la $j$-ème colonne à la $i$-ème colonne. Le $i$-ème coefficient de la diagonale devient alors $2 b_{ij} + b_{jj} = 2 b_{ij} \neq 0$.
 
 \item Si, à son tour, un tel indice $j$ n'existe pas, on peut procéder à la $i+1$-ème itération car la matrice est déjà de la forme \eqref{eq:7} (avec $i+1$ à la place de $i$).
  
\item Pour chaque $j \in \{i+1,\dots,n\}$:  on additionne $-b_{ij}/b_{ii}$ fois la $i$-ème ligne sur la $j$-ème ligne et on additionne $-b_{ij}/b_{ii}$ fois la $i$-ème colonne sur la $j$-ème colonne. Ceci permet d'annuler les coefficients à droite et sous le coefficient $b_{ii}$. On peut alors poursuivre à l'étape $i+1$.
\end{itemize}    

Remarquons que chaque opération est faite à la fois sur les lignes et sur les colonnes. Ceci garantit que la matrice résultante reste symétrique. De plus, les opérations sont faites sur les lignes et colonnes d'indices $j \geq i$, laissant intacte la forme de la matrice \eqref{eq:7}.

\end{algorithm}


\begin{example}
  \label{exe:15}
  Soit $V$ une espace vectoriel sur $\Q$ de dimension $3$ muni d'une forme bilinéaire symétrique $〈,〉$. Soit $B = \{v_1,v_2,v_3\}$ une base de $V$ et 
  \begin{displaymath}
    A^{\pscal{.}}_B =
    \begin{pmatrix}
      1 & 0 & 2\\
      0 & 3 & 4\\
      2 & 4 & 0 
    \end{pmatrix}
  \end{displaymath}
Le but est de trouver une  base orthogonale de $V$. 

En utilisant notre algorithme on trouve 
\begin{displaymath}
  P = 
  \begin{pmatrix}
    
1 & 0 & -2 \\
0 & 1 & -\frac{4}{3} \\
0 & 0 & 1
  \end{pmatrix}
\end{displaymath}
telle que 
\begin{displaymath}
  P^T \cdot A^{\pscal{.}}_B \cdot P =
  \begin{pmatrix}
    1 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & -\frac{28}{3}
  \end{pmatrix}. 
\end{displaymath}
Alors $B' = \{v_1,v_2,-2v_1 -(4/3) v_2 + v_3\}$ est une base orthogonale de $V$. 
\end{example}





\subsection*{Exercices}

\begin{enumerate}
\item Montrer que $\cong$ est une relation d'équivalence sur l'ensemble des matrices $K^{n\times n}$. 
\item Est-ce que la matrice 
  \begin{displaymath}
    \begin{pmatrix}
      0 & 1 & 0 \\
      1 & 0 & 1\\
      0 & 1 & 0 
    \end{pmatrix} \in \Z_2^{3\times 3} 
  \end{displaymath}
  est congruente à une matrice diagonale? \emph{Indice : voir l'exercice~\ref{item:1}. de la section~\ref{sec:orthogonalite}.}  
\item Soit $V$ un espace vectoriel  sur un corps $K$ de dimension finie muni d'une forme bilinéaire symétrique $\pscal{.}$. Soit $B = \{v_1,\dots,v_n\}$ une base de $V$. Montrer que $A_B^{\pscal{.}} \in K^{n \times n}$ est congruente à une matrice diagonale si  et seulement si $V$ possède une base orthogonale. 

\item Soit $K$ un corps de caractéristique $2$ et soit $V$ un espace vectoriel sur $K$ de dimension finie muni d'une forme bilinéaire symétrique non-nulle.  Soit 
  \begin{displaymath}
    C =
    \begin{pmatrix}
      0 & 1 \\
      1 & 0 
    \end{pmatrix}.
  \end{displaymath}
  \begin{enumerate}[a)]
  \item Soit $\dim(V) = 2$. Montrer que $V$ ne possède pas de base orthogonale si et seulement s'il existe une base $B$ de $V$ telle que $A_B^{\pscal{.}} = C$. 
  \item Soit $\dim(V) = n$. Montrer que $V$ ne possède pas de base orthogonale si et seulement s'il existe une base $B$ de $V$ telle que 
    \begin{displaymath} A_B^{\pscal{.}} = 
      \begin{pmatrix}
        d_1 \\
        & d_2 \\
        & & \ddots \\
        & &   & d_k \\
        & &   &  & C \\
        & &   &  & & \ddots \\
        & &   &  & & & C \\
      \end{pmatrix}
    \end{displaymath}
    et $d_1,\dots,d_k = 0$, et le nombre de $C$ n'est pas égal à zéro. 
  \end{enumerate}
\item Modifier l'algorithme~\ref{alg:1} tel qu'il soit aussi correct pour des corps de caractéristique $2$.  Soit l'algorithme découvre que la matrice symétrique $A \in K^{n \times n}$ n'est pas congruente à une matrice diagonale, soit l'algorithme calcule une matrice diagonale congruente à $A$. 
\item Comment peut-on déterminer si un espace vectoriel de dimension finie muni d'une forme bilinéaire symétrique  possède une base orthogonale? Décrire très brièvement une méthode. 
 
\item Soit $V$ un espace euclidien de dimension $n$. Montrer que $V$ possède une base $B$ telle que pour tout $x,y \in V$
  \begin{displaymath}
    \pscal{x,y} = [x]_B \cdot [y]_B, 
  \end{displaymath}
où $ [x]_B \cdot [y]_B$  dénote la forme bilinéaire standard de $\R^n$ entre $[x]_B$ et $ [y]_B$ .  

\end{enumerate}




\section{Le théorème de Sylvester}
\label{sec:le-theoreme-de}




Soit $V$ un espace vectoriel de dimension finie sur un corps $K$, $\mathrm{Char}(K) \neq 2$, muni d'une forme bilinéaire symétrique. Nous avons vu (théorème~\ref{thr:5}) que $V$ possède une base orthogonale. Supposons que cette base est $B = \{v_1,\dots,v_n\}$ et considérons $x = \sum_i \alpha_i v_i \in V$ et $y = \sum_i \beta_i v_i \in V$. La forme bilinéaire  s'écrit 
\begin{eqnarray*}
  \pscal{x,y} & =  & \sum_{i, j} \alpha_i \beta_j \pscal{v_i,v_j} \\
              & = & \sum_i \alpha_i \beta_i \pscal{v_i,v_i} \\
               & = & [x]_B^T 
                    \begin{pmatrix}
                      c_1 & & \\
                                     & \ddots & \\
                                     & & c_n
                    \end{pmatrix} [y]_B
\end{eqnarray*}
où $c_i = \pscal{v_i,v_i}$ pour tout $i$. 
Si $K = \R$ on peut ordonner la base afin d'avoir $c_1,\dots,c_r>0$, $c_{r+1},\dots,c_s < 0$ et $c_{s+1},\dots,c_n = 0$.



Maintenant soit $K = \R$  et $A \in \R^{n \times n}$ symétrique. Le Corollaire~\ref{co:4} implique qu'il existe une matrice inversible $P ∈ ℝ^{n \times n}$ tel que $P^T A P = D$ où $D$ est une matrice diagonale. Si on échange deux colonnes de $P$ et note $P'$ la nouvelle matrice obtenue, alors $P'^T A P' = D'$, où $D'$ est obtenue de $D$ en échangeant les éléments diagonaux correspondants. Alors on peut trouver une matrice inversible $P ∈ ℝ^{n ×n}$ telle que 
\begin{equation}
  \label{eq:29}
  P^T A {P} =
    \begin{pmatrix}
      c_1\\
      & \ddots \\
      && c_n
    \end{pmatrix}. 
\end{equation}
où les  $c_i$ sont ordonnés de sorte que  $c_1,\dots, c_r >0$, $c_{r+1},\dots ,c_s<0$ et $c_{s+1},\dots , c_n = 0$. En multipliant les premières $s$ colonnes de $P$ par $1/ \sqrt{|c_i|}$ on obtient en fait une factorisation~\eqref{eq:29} telle que 
 $c_1,\dots, c_r =1$, $c_{r+1},\dots ,c_s=-1$ et $c_{s+1},\dots , c_n = 0$. 

Alors on trouve  $P ∈ ℝ^{n ×n}$ inversible telle que 
\begin{equation}
  \label{eq:5}
  P^T A P =
  \begin{pmatrix}
    1 &   \\
      & \ddots &  \\
      &        & 1 \\
      &        &  & -1 \\
      &        &  &    & \ddots \\
      &        &  &    &        & -1 \\
      &        &  &    &        &    & 0 \\
      &        &  &    &        &    &   & \ddots  \\
      &        &  &    &        &    &   &        & 0  \\

  \end{pmatrix}.
\end{equation}


\begin{definition}
  \label{def:37}
  Pour un espace vectoriel sur $ℝ$ de dimension finie, on appelle une base $B$ de $V$ telle que $A_B^{\pscal{,}}$ a la forme décrite en~\eqref{eq:5} une \emph{base de Sylvester}. 
\end{definition}

Nous allons maintenant démontrer, que les nombres $r$ et $s$ sont invariants par rapport au choix de la base $B$ de $V$. 


\begin{definition}
  \label{def:11}
  Le sous espace $V_0 = \{v \in V \colon \pscal{v,x} = 0 \text{ pour tout } x \in V\}$ est appelé l'\emph{espace de nullité} de la forme bilinéaire symétrique $\pscal{.}$. 
\end{definition}

\begin{theorem}
  \label{thr:9}
  Soit $V$
  un espace vectoriel de dimension finie sur un corps $K$
  de caractéristique $\neq 2$
  et soit $V$
  muni d'une forme bilinéaire symétrique. Soit $B = \{v_1,\dots,v_n\}$
  une base orthogonale de $V$.
  La dimension $\dim(V_0)$
  est égale au nombre d'indices $i$ tel que $\pscal{v_i,v_i}=0$.
\end{theorem}

\begin{proof}
  Nous utilisons la notation d'au-dessus et écrivons 
  \begin{displaymath}
    \pscal{v,x} =   [v]_B^T
                    \begin{pmatrix}
                      c_1 & & \\
                                     & \ddots & \\
                                     & & c_n
                    \end{pmatrix} [x]_B. 
  \end{displaymath}
Cette expression est égale à zéro pour tout $x ∈V$  si et seulement si $\left([v]_B\right)_i = 0$ pour tout $i$ tel que $c_i \neq 0$. Ceci démontre que $\{ v_i \colon \pscal{v_i,v_i}=0\}$ est une base de l'espace de nullité. 
\end{proof}

\begin{definition}
  La dimension de l'espace de nullité $\dim(V_0)$ est appelé l'\emph{indice de nullité} de la forme bilinéaire symétrique. 
\end{definition}

\begin{theorem}[Théorème de Sylvester] 
\label{thr:10}
Soit $V$
un espace vectoriel de dimension finie sur $\R$
muni d'une forme bilinéaire symétrique.
Il existe un nombre entier $r ≥0$ tel que, pour chaque base orthogonale 
  $B = \{v_1,\dots,v_n\}$ de $V$, 
 exactement $r$ des indices $i$ satisfont $\pscal{v_i,v_i}>0$.
\end{theorem}


\begin{proof}
  Soient $\{v_1,\dots,v_n\}$
  et $\{w_1,\dots,w_n\}$ des
  bases orthogonales de $V$ ordonnées 
  telles que $\pscal{v_i,v_i} >0 $
  si $1 \leq i \leq r$,
  $\pscal{v_i,v_i} <0 $
  si $r+1 \leq i \leq s$
  et $\pscal{v_i,v_i} =0 $
  si $s+1 \leq i \leq n$.
  De même $\pscal{w_i,w_i} >0 $
  si $1 \leq i \leq r'$,
  $\pscal{w_i,w_i} <0 $
  si $r'+1 \leq i \leq s'$
  et $\pscal{w_i,w_i} =0 $ si $s'+1 \leq i \leq n$.



  On démontre que $v_1,\dots,v_r,w_{r'+1},\dots w_n$
  est linéairement indépendant. Ça implique que $r + n-r' \leq n$
  et alors $r \leq r'$.
  Parce que l'argument est symétrique on peut conclure que $r = r'$.

  Si $v_1,\dots,v_r,w_{r'+1},\dots w_n$ est linéairement dépendant, il existe des scalaires $x_1,\dots x_r$ et $y_{r'+1},\dots y_n$ respectivement  non tous égaux à zéro tels que 
  \begin{displaymath}
    x_1 v_1 + \cdots + x_r v_r = y_{r'+1} w_{r'+1} + \cdots y_n w_n 
  \end{displaymath}
et ça implique, car les $v_i$ et respectivement les $w_i$  sont orthogonaux entre eux, 
\begin{displaymath}
   x_1^2\pscal{v_1,v_1} + \cdots + x_r^2 \pscal{v_r,v_r} = y_{r'+1}^2 \pscal{w_{r'+1},w_{r'+1}} + \cdots y_n^2 \pscal{w_n,w_n} 
\end{displaymath}
Les $\pscal{v_i,v_i}$ à gauche sont strictement positifs. Les $\pscal{w_i,w_i} $ à droite sont négatifs ou nuls. Il suit que $x_1=0,\dots,x_r = 0$ et, comme les $w_i$ sont linéairement indépendants, on a également $y_{r'+1}=0,\dots,y_n=0$. 
\end{proof}


\begin{definition}
  \label{def:12}
  L'entier $r$ du théorème de Sylvester est appelé l'\emph{indice de positivité} de la forme bilinéaire symétrique. 
\end{definition}



\begin{example}
  \label{exe:11}
  Trouver une base de Sylvester de $ℝ^4$ et les indices de nullité et de positivité de la forme bilinéaire symétrique $x^TAy$ où  
  \begin{displaymath}
A = 
    \begin{pmatrix}
      2 & 4 & 6 \\
      4 & 4 & 3 \\
      6 & 3 & 1 
    \end{pmatrix}
  \end{displaymath}
On utilise des transformations élémentaires sur les colonnes et les mêmes sur les lignes tour à tour en alternant. 

\noindent Les transformations élémentaires sur les colonnes sont représentées par 
\begin{displaymath}
P_1 = 
  \left[\begin{matrix}1 & -2 & -3\\0 & 1 & 0\\0 & 0 & 1\end{matrix}\right]
\end{displaymath}
et transforment la matrice $A$ en 
\begin{displaymath}
   \begin{pmatrix}
      2 & 4 & 6 \\
      4 & 4 & 3 \\
      6 & 3 & 1 
    \end{pmatrix} \cdot  \left[\begin{matrix}1 & -2 & -3\\0 & 1 & 0\\0 & 0 & 1\end{matrix}\right] = \left[\begin{matrix}2 & 0 & 0\\4 & -4 & -9\\6 & -9 & -17\end{matrix}\right]. 
\end{displaymath}
Alors 
\begin{displaymath}
  P_1^T\cdot A \cdot P = \left[\begin{matrix}2 & 0 & 0\\0 & -4 & -9\\0 & -9 & -17\end{matrix}\right]
\end{displaymath}
Avec 
\begin{displaymath}
  P_2 = \left[\begin{matrix}1 & 0 & 0\\0 & 1 & -\frac{9}{4}\\0 & 0 &1\end{matrix}\right]
\end{displaymath}
on obtient 
\begin{displaymath}
  P_2^TP_1^T A P_1 P_2 = \left[\begin{matrix}2 & 0 & 0\\0 & -4 & 0\\0 & 0 & \frac{13}{4} \end{matrix}\right]
\end{displaymath}
L'indice de nullité est zéro et l'indice de positivité est $2$. Le produit $P_1\cdot P_2$ est égal à 
\begin{displaymath}
  P_1 \cdot P_2 = \left[\begin{matrix}1 & -2 & \frac{3}{2}\\0 & 1 & -\frac{9}{4}\\0 & 0 & 1\end{matrix}\right]
\end{displaymath}
En divisant les colonnes de $P$ par $\sqrt{2},\sqrt{4}$ et $\sqrt{13/4}$ respectivement, on obtient une transformation $P$ telle que $P^TAP =
\begin{pmatrix}
  1& \\
  & 1 & \\
  & & -1
\end{pmatrix}$. 
\end{example}
Les colonnes de $P$ sont une base de Sylvester. 



\subsection*{Exercices} 

\begin{enumerate}
\item Démontrer, à l'aide des théorèmes \ref{thr:9} et \ref{thr:10}, que l'indice de négativité (l'entier $s$ de l'équation \eqref{eq:29}) ne dépend lui aussi pas de la base choisie.
\item Déterminer l'indice de nullité et l'indice de positivité des 
formes bilinéaire symétriques 
 définies par les matrices suivantes
  \begin{displaymath}
    \begin{pmatrix}
      1 & 2 \\
      2 & -1
    \end{pmatrix}
    , \,
    \begin{pmatrix}
      1 & 1 \\
      1 & 1
    \end{pmatrix}, \, 
    \begin{pmatrix}
      2 & 4 & 2 \\
      4 & 3 &  1 \\ 
      2 & 1 &1
    \end{pmatrix}. 
  \end{displaymath}

\item Soit $V$ un espace vectoriel de dimension finie sur $\R$ et soit $\pscal{.}$ une forme bilinéaire symétrique  sur $V$.  Montrer que $V$ admet une décomposition en somme directe 
  \begin{displaymath}
    V_0 \oplus V^+ \oplus V^-
  \end{displaymath}
où $V_0$ est l'espace de nullité et $V^+$ et $V^-$ sont des sous-espaces tels que 
\begin{displaymath}
  \pscal{v,v} >0 \text{ pour tout } v \in V^+ \setminus\{0\}
\end{displaymath}
et  
\begin{displaymath}
  \pscal{v,v} <0 \text{ pour tout } v \in V^- \setminus\{0\}. 
\end{displaymath}
\end{enumerate}






\chapter{Produits scalaires et hermitiens}
\label{sec:le-case-reel}




\begin{definition}
  \label{def:4}
  Soit $V$ un espace vectoriel sur $\R$ muni 
  d'une forme bilinéaire symétrique. 
  La forme bilinéaire symétrique  est définie positive si $\pscal{v,v} \geq 0$ pour tout $v \in V$, et si $\pscal{v,v}>0$ lorsque $v \neq 0$. Une forme bilinéaire symétrique définie positive est un \emph{produit scalaire}. 
\end{definition}

\begin{framed}\noindent 
  Pour  ce chapitre~\ref{sec:le-case-reel}, sauf pour Chapitre~\ref{sec:form-sesq-et},  $V$
  est toujours un espace vectoriel sur $\R$
  muni d'un produit scalaire. 
On appelle un  espace vectoriel sur $\R$ muni d'un produit scalaire  un \emph{espace euclidien}.  
\end{framed}


\begin{example}
  \label{exe:3}
  Soit $V = \R^n$. La forme bilinéaire symétrique
  \begin{displaymath}
    \pscal{u,v} = \sum_{i=1}^n u_iv_i 
  \end{displaymath}
  est un produit scalaire, appelé le \emph{produit scalaire ordinaire}. 
  Aussi, la forme bilinéaire  de l'exemple~\ref{ex:2} est un produit scalaire. 
\end{example}

\begin{definition}
  \label{def:5}
  Soit $\pscal{,}$ un produit scalaire. La \emph{longueur} ou la \emph{norme} d'un élément $v \in V$ est le nombre 
  \begin{displaymath}
    \| v \| = \sqrt{\pscal{v,v}}.
  \end{displaymath}
  Un élément $v \in V$ est un \emph{vecteur unitaire} si $\|v\| = 1$. 
\end{definition}





\begin{proposition}
  \label{prop:2}
  Pour $v \in V$ et $\alpha \in \R$ on a
  \begin{displaymath}
    \| \alpha \,v \| = |\alpha| \, \|v\|. 
  \end{displaymath}
\end{proposition}



\begin{proof}
  \begin{eqnarray*}    
   \| \alpha \,v \| & = &  \sqrt{\pscal{\alpha \, v, \alpha \, v} } \\
                    & = & \sqrt{\alpha^2 \pscal{v,v}} \\
                    & = & |\alpha | \, \|v\|. 
  \end{eqnarray*}
\end{proof}

\begin{proposition}[Théorème de Pythagore]
\label{prop:4}
Si $v$ et $w$ sont perpendiculaires 
\begin{displaymath}
  \|v+w\|^2 = \|v\|^2 + \|w\|^2. 
\end{displaymath}  
\end{proposition}


\begin{proof}
  \begin{eqnarray*}
     \|v+w\|^2 &= & \pscal{v+w,v+w} \\
               & = & \pscal{v,v+w} + \pscal{w,v+w} \\
               & = & \pscal{v,v} + \pscal{v,w} + \pscal{w,v} + \pscal{w,w} \\
               & = & \|v\|^2 + \|w\|^2
  \end{eqnarray*}
\end{proof}

\begin{proposition}[Règle du parallélogramme] Pour tous $v$ et $w$, on a 
  \begin{displaymath}
    \|v+w\|^2 + \|v-w\|^2 = 2 \|v\|^2 + 2 \|w\|^2.  
  \end{displaymath}  
\end{proposition}



Soit $V$ un espace vectoriel sur un corps $K$ muni d'un produit scalaire  $\pscal{,}$. 
Si $w ≠ 0$ est un élément de $V$, alors  $\pscal{w,w} >0$. Pour  tout $v \in V$, il existe un élément unique $\alpha \in K$ tel que $\pscal{w,v -\alpha\, w} = 0$. 

En fait,
\begin{displaymath}
  \pscal{w,v -\alpha\, w} = \pscal{w,v} - \alpha \pscal{w,w}. 
\end{displaymath}
Alors $\pscal{w,v -\alpha\, w} = 0$ si et seulement si $\alpha = \pscal{v,w} / \pscal{w,w}$. 


\begin{definition}
  \label{def:6}
  Soit $V$ un espace euclidien. 
  Soit $w \in V \setminus \{0\}$. Pour $v \in V$, soit $\alpha = \pscal{v,w} / \pscal{w,w}$.  Le nombre $\alpha$ est la \emph{composante} de $v$ sur $w$, ou \emph{le coefficient de Fourier de $v$ relativement à $w$}. Le vecteur $\alpha \, w$ s'appelle la \emph{projection} de $v$ sur $w$. 
\end{definition}


\begin{example}
  \label{exe:5}
  Soit $V$ l'espace vectoriel de l'exemple~\ref{ex:2} et $f(x) = \sin kx$, où $k \in \N_{>0}$. Alors
  \begin{displaymath}
   \|f\| = \sqrt{\pscal{f,f}} =  \sqrt{\int_0^{2\pi} \sin^2 kx \, dx} = \sqrt{\pi}
  \end{displaymath}
Si $g$ est une fonction quelconque, continue sur $[0,2\,\pi]$, le coefficient de Fourier de $g$ relativement à $f$ est 
\begin{displaymath}
  \pscal{f,g} /   \pscal{f,f}  = \frac{1}{\pi} \int_0^{2\, \pi} g(x) \sin kx \,dx. 
\end{displaymath}
\end{example}



\begin{theorem}[Inégalité de Cauchy-Schwarz]
  Pour tous $v,w \in V$, on a  
  \begin{displaymath}
    |\pscal{v,w}| \leq \|v\| \, \|w\|.
  \end{displaymath}
\end{theorem}


\begin{proof}
  Si $w=0$, les deux termes de cette inégalité sont nuls et elle devient évidente. Supposons maintenant que $w \neq 0$. Si $\alpha = \pscal{v,w} / \pscal{w,w}$ est la composante de $v$ sur $w$, $v - \alpha w$ est perpendiculaire à $w$, donc aussi à $\alpha\,w$. D'après le théorème de Pythagore, on trouve 
  \begin{eqnarray*}
    \|v\|^2 & = & \|v - \alpha \,w\|^2 + \|\alpha \, w \|^2 \\
            & ≥ &   \alpha^2 \|w\| ^2 \\ 
            & = &     \pscal{v,w}^2    / \|w\| ^2. 
  \end{eqnarray*}
Cela implique 
\begin{displaymath}
   \left| \pscal{v, w } \right| \leq \|v\| \|w\|.
\end{displaymath}
\end{proof}




\begin{theorem}[Inégalité triangulaire]
  \label{thr:1}
  Si $v,w \in V$. 
  \begin{displaymath}
    \|v+w\| \leq \|v\| + \|w\|. 
  \end{displaymath}
\end{theorem}


\begin{proof}
  \begin{eqnarray*}
    \|v+w\|^2 & =     & \|v\|^2 + 2 \pscal{v,w} + \|w\|^2 \\
              & \leq & \|v\|^2 + 2 \|v\|\,\|w\| + \|w\|^2 \\
              & = & (\|v\| + \|w\|)^2,
  \end{eqnarray*}
en recourant à l'inégalité de Cauchy-Schwarz. 
\end{proof}


\begin{lemma}
  \label{lem:2}
  Soit $V$ un espace euclidien  et 
  soient $v_1,\dots,v_n$ des éléments de V, deux à deux orthogonaux, tels que $v_i \neq 0$ pour tout $i$, et soit $a_1,\dots,a_n \in \R$.
  %Soit $\alpha_i = \pscal{v,v_i}/\pscal{v_i,v_i}$ la composante de $v$ sur $v_i$, alors le vecteur 
  Le vecteur 
\begin{displaymath}
  v - a_1v_1- \cdots - a_n v_n\, % \text{ où  }\, a_i \in ℝ 
\end{displaymath}
est perpendiculaire à tous les $v_1,\dots,v_n$ si et seulement si $a_i$ est la composante de $v$ sur $v_i$, c'est-à-dire $a_i=  \pscal{v,v_i}/ \pscal{v_i,v_i}$ pour tout $i$. 
\end{lemma}

\begin{proof}
  Pour le vérifier, il suffit d'en faire le produit scalaire avec $v_j$ pour tout $j$. Tous les termes $\pscal{v_i,v_j}$ donnent zéro si $i\neq j$. Le reste
  \begin{displaymath}
    \pscal{v,v_j} - a_j\pscal{v_jv_j}
  \end{displaymath}
s'annule si et seulement si $a_j = \pscal{v,v_j}/\pscal{v_jv_j}$. 
\end{proof}


 % \begin{definition}
 %   \label{def:3}
 %   Soit $V$ un espace vectoriel muni d'un produit scalaire. Soit $\{v_1,\dots,v_n\}$ une base de $V$. On dira que cette base est \emph{orthogonale} si $\pscal{v_i,v_j}=0$ pour tout $i \neq j$. 
 % \end{definition}

\begin{notation}
  Soient $V$ un espace vectoriel et $v_1,\dots,v_n \in V$. Le sous-espace engendré par $v_1,\dots,v_n$ est dénoté par  $\spa\{v_1,\dots,v_n\}$. 
\end{notation}



\begin{theorem}[Le procédé d'orthogonalisation de Gram-Schmidt]
  \label{thr:2}
  Soient $V$ un espace euclidien et  $\{v_1,\dots,v_n\} \subseteq V$
  un ensemble libre.  
  Il existe un ensemble libre orthogonal $\{u_1,\dots,u_n\}$
  de $V$
  tel que pour tout $i$, 
  $\{v_1,\dots,v_i\}$
  et $\{u_1,\dots,u_i\}$ engendrent le même sous-espace de $V$.
\end{theorem}


\begin{proof}
  On montre le théorème par induction.  On met $u_1 = v_1$
  et on suppose qu'on a construit $\{u_1,\dots,u_{i-1}\}$
  pour $i \geq 2$.
  L'ensemble $\{u_1,\dots,u_{i-1},v_i\}$
  est libre et une base du sous-espace engendré
  par $\{v_1,\dots,v_i\}$.
  On met  
  \begin{displaymath}
    u_i = v_i - \alpha_{1,i}  u_1 - \cdots - \alpha_{i-1,i} u_{i-1}
  \end{displaymath}
  où les $\alpha_{j,i}$
  sont les composantes de $v_i$
  sur $u_j$.
  Comme ça 
  \begin{eqnarray*}
    \spa \{u_1,\dots,u_i\} & = & \spa\{u_1,\dots,u_{i-1},v_i\} \\
                           &=  &\spa\{v_1,\dots,v_i\}.
  \end{eqnarray*}
  Surtout $\{u_1,\dots,u_i\}$ est un ensemble orthogonal.  
\end{proof}



\begin{exercise}
  \label{exe:6}
  Est-ce qu'il faut vraiment supposer que le produit scalaire
  $\pscal{.}$
  soit réel et défini positif et sur $\R$ pour ce procédé? Peux-tu imaginer une condition plus
  faible et satisfaite par la forme bilinéaire symétrique  qui permet
  le procédé de Gram-Schmidt? 
\end{exercise}



\begin{definition}
  \label{def:7}
  Une base $\{u_1,\dots,u_n\}$
  d'un espace euclidien est \emph{orthonormale} si elle est
  orthogonale et se compose de vecteurs tous unitaires.
\end{definition}




\begin{corollary}
  \label{co:1}
  Soit $V$
  un espace euclidien de dimension finie. Supposons $V \neq
  \{0\}$. $V$ possède alors une base orthonormale.
\end{corollary}

\begin{proof}
  Soient $\{v_1,\dots,v_n\}$ une base de $V$ et $\{u_1,\dots,u_n\}$ le résultat du procédé Gram-Schmidt appliqué à $\{v_1,\dots,v_n\}$. Alors $\{u_1/ \|u_1\|,\dots,u_n/\|u_n\|\}$ est une base orthonormale de $V$. 
\end{proof}


\begin{example}
\label{exe:7}
Trouver une base orthonormale de l'espace vectoriel engendré par 
\begin{displaymath}
  \begin{pmatrix}
    1\\1\\0\\1
  \end{pmatrix}, 
  \begin{pmatrix}
    1\\-2\\0\\0
  \end{pmatrix}
  \text{ et } 
  \begin{pmatrix}
    1\\0\\-1\\2
  \end{pmatrix}
\end{displaymath}

Notons $A,B$ et $C$ les vecteurs. Soit $A'=A$ et 
\begin{displaymath}
  B' = B - \frac{A'\cdot B}{A'\cdot A'} \cdot A'
\end{displaymath}
On trouve 
\begin{displaymath}
  B' = \frac{1}{3}
  \begin{pmatrix}
    4\\-5\\0\\1
  \end{pmatrix}
\end{displaymath}
On calcule 
\begin{displaymath}
  C' = C - \frac{A'\cdot C}{A'\cdot A'} \cdot A' - \frac{B'\cdot C}{B'\cdot B'} \cdot B'
\end{displaymath}
et on trouve 
\begin{displaymath}
  C' = \frac{1}{7}
  \begin{pmatrix}
    -4\\-2\\-7\\6
  \end{pmatrix}  
\end{displaymath}
La base orthonormale est
\begin{displaymath}
  A' / \|A'\| = \frac{1}{\sqrt{3}}
  \begin{pmatrix}
    1\\1\\0\\1
  \end{pmatrix},
    B'/ \|B'\| = \frac{1}{\sqrt{42}}
  \begin{pmatrix}
    4\\-5\\0\\1
  \end{pmatrix}
\text{ et }
C'/ \|C'\| = \frac{1}{\sqrt{105}}
  \begin{pmatrix}
    -4\\-2\\-7\\6
  \end{pmatrix}  
\end{displaymath}
\end{example}

\begin{corollary}
  \label{co:2}
  Soit $A \in \R^{m\times n}$ une matrice de rang (colonne) plein. On peut factoriser $A$ comme 
  \begin{displaymath}
    A = A^* \cdot R
  \end{displaymath}
où les colonnes de  $A^* \in \R^{m\times n}$ sont deux à deux orthonormales et $R \in \R^{n \times n}$ est une matrice triangulaire supérieure dont les valeurs diagonales sont positives. 
\end{corollary}

\begin{proof}
Comme $\rank(A) = n$, les colonnes de $A$ sont libres ; dès lors on peut appliquer le procédé de Gram-Schmidt à $\{a_1, \dots , a_n \} $ où $a_j$ désigne la j-ième colonne de $A$. On obtient alors une base orthogonale $B=  \{a'_1, \dots , a'_n\}$ avec la relation :
\begin{displaymath}
a_1=a'_1
~~,~~
a_j = \sum_{i=1}^{j-1} \alpha_{i,j} a'_i +a'_j
\end{displaymath}
pour tout $j \in \{2, \dots,n\}$, et où $\alpha_{i,j} $ est le coefficient de Fourier de $a_j$ relativement à $a'_i$.
Grâce à ce procédé, on a pu écrire $a_j$ comme une combinaison linéaire de $\{a'_1, \dots , a'_n\} $. On peut représenter cela avec un produit matrice-vecteur:
\begin{displaymath}
a_j= (a'_1 ~ \dots ~ a'_n)
\begin{pmatrix}
\alpha_{1,j}\\\alpha_{2,j}\\\vdots\\\alpha_{j-1,j}\\1\\0\\\vdots\\0\\
\end{pmatrix}.  
%~~~~~~A' \in \R^{m\times n}
%~~~~~~s_j \in \R^{n}
\end{displaymath}

En posant 
\begin{displaymath}
S = %:= (s_1 ~ \dots ~ s_n)= 
\begin{pmatrix}
1 & \alpha_{1,2} & \alpha_{1,3} & \cdots & \alpha_{1,n-1} & \alpha_{1,n}\\
0 & 1 & \alpha_{2,3} & \cdots & \alpha_{2,n-1} & \alpha_{2,n} \\
\vdots && \ddots &&& \vdots \\
\vdots &&& \ddots &&\vdots\\
0 & \cdots & \cdots&0& 1 & \alpha_{n-1,n}\\
0 & 0 & \cdots &\cdots & 0 & 1
\end{pmatrix} \in \R^{n \times n}, 
\end{displaymath}
on a, par les propriétés du produit matriciel,
\begin{displaymath}
A = A'S,  %=(A'_1s_1 ~\dots ~ A'_1s_n)=(a_1 ~ \dots ~ a_n)=A
\end{displaymath}
où $A' = (a'_1 \dots a'_n)$. Il nous faut encore normaliser les colonnes de la matrice $A'$. Pour cela, on définit les deux matrices diagonales suivantes:
\begin{displaymath}
D=\begin{pmatrix}
1/\|a'_1\| & & \\
& \ddots & \\
& & 1/\|a'_n\|
\end{pmatrix} , \quad 
D^{-1}=\begin{pmatrix}
\|a'_1\| & & \\
& \ddots & \\
& & \|a'_n\|
\end{pmatrix} 
~~ D,D^{-1} \in \R^{n \times n}
\end{displaymath}
En posant $A^*=A'D$ et $R=D^{-1}S$ on obtient:
\begin{displaymath}
A=A^* R
\end{displaymath}
où $A^* \in \R^{m\times n}$ et $R \in \R^{n \times n}$ sont des matrices qui vérifient les propriétés de l'énoncé. 
\end{proof}


\begin{example}
  \label{exe:8}
  Trouver une  factorisation $Q,R$ du Corollaire~\ref{co:2} de la matrice 
  \begin{displaymath}
    \left(\begin{matrix}1 & 1 & 1\\1 & -2 & 0\\0 & 0 & -1\\1 & 0 & 2\end{matrix}\right)
  \end{displaymath}

On trouve 
\begin{displaymath}
  \left[\begin{matrix}1 & 1 & 1\\1 & -2 & 0\\0 & 0 & -1\\1 & 0 & 2\end{matrix}\right]
= \left[\begin{matrix}1 & \frac{4}{3} & - \frac{4}{7}\\1 & - \frac{5}{3} & - \frac{2}{7}\\0 & 0 & -1\\1 & \frac{1}{3} & \frac{6}{7}\end{matrix}\right] 
\left[\begin{matrix}1 & - \frac{1}{3} & 1\\0 & 1 & \frac{3}{7}\\0 & 0 & 1\end{matrix}\right]
\end{displaymath}
et alors
\begin{displaymath}
   \left[\begin{matrix}1 & 1 & 1\\1 & -2 & 0\\0 & 0 & -1\\1 & 0 & 2\end{matrix}\right] = 
\left[\begin{matrix}\frac{1}{\sqrt{3}} & \frac{2 \sqrt{42}}{21} & - \frac{4 }{\sqrt{105}}\\\frac{1}{\sqrt{3}} & - \frac{5}{\sqrt{42}} & - \frac{2 }{\sqrt{105}}\\0 & 0 & -\frac{\sqrt{105}}{15}\\\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{42}} & \frac{2 \sqrt{105}}{35}\end{matrix}\right] 
\left[\begin{matrix}\sqrt{3} & - \frac{\sqrt{3}}{3} & \sqrt{3}\\0 &\frac{\sqrt{42}}{3} & \frac{\sqrt{42}}{7}\\0 & 0 & \frac{\sqrt{105}}{7}\end{matrix}\right]
\end{displaymath}
\end{example} 




\begin{theorem}[Inégalité de Bessel]
  \label{thr:4}
  Si $v_1,\dots,v_n$ sont des vecteurs unitaires deux à deux orthogonaux et si $\alpha_i = \pscal{v,v_i}$ sont les coefficients de Fourier de $v$ relativement à $v_i$ alors 
  \begin{displaymath}
    \sum_{i=1}^n \alpha_i^2 \leq \|v\|^2. 
  \end{displaymath}
\end{theorem}

\begin{proof}
  \begin{eqnarray*}
    0 & \leq & \pscal{v - \sum_{i=1}^n \alpha_i v_i , v - \sum_{i=1}^n \alpha_i v_i} \\
     & = & \pscal{v,v} - 2 \cdot \sum \alpha_i \pscal{v,v_i} + \sum \alpha_i^2 \\
    & = & \pscal{v,v}  - \sum \alpha_i^2 \\
  \end{eqnarray*}
\end{proof}



\subsection*{Exercices}

\begin{enumerate}

\item Soit $V$ un espace vectoriel sur $ℝ$ de dimension fini, muni d'une forme bilinéaire $\pscal{.}$. Soit $B = \{b_1,\dots,b_n\}$ une base orthogonale et  $U = \spa \{ b_i :  i=1,\dots,n, \, 〈b_i,b_i〉 >0\}$. Montrer que $\pscal{.}$ restreint à $U$ est un produit scalaire du sous-espace $U$. 
\item Soient $V$ un espace vectoriel muni d'une forme bilinéaire symétrique $\pscal{.}$ et $\{v_1,\dots,v_n\} \subseteq V$ un ensemble de
vecteurs deux à deux orthogonaux. 
  \begin{enumerate}[a)]
  \item   Montrer que $\{v_1,\dots,v_n\}$ est un ensemble libre si pour tout $i$, 
 $\pscal{v_i,v_i} \neq  0$.  
\item  Donner un contre-exemple ou une démonstration de la réciproque. 
\end{enumerate}


\item Considérant l'exemple~\ref{ex:2}, montrer que l'ensemble 
  \begin{displaymath}
    \{1,\sin x, \cos x, \sin(2x), \cos(2x), \sin(3x), \cos(3x), \dots\}
  \end{displaymath}
 est un ensemble de 
vecteurs deux à deux orthogonaux. 
\item Trouver la factorisation $Q\cdot R$  du Corollaire~\ref{co:2} de la matrice 
  \begin{displaymath}
    \begin{pmatrix}
      1 & 1 & 0 &0 \\
      0& 1 & 1 & 0\\
      0 & 0 & 1 & 1\\
      1 & 0 & 0 & 1\\
    \end{pmatrix}
  \end{displaymath}
  Trouver la factorisation de la matrice $n\times n$ 
  \begin{displaymath}
    \begin{pmatrix}
      1 & 1 & 0 & 0 & \cdots & 0\\
      0 & 1 & 1 & 0 & \cdots & 0 \\
      && \vdots &&\\
      0 & \cdots & \cdots&0& 1 & 1\\
      1 & 0 & \cdots &\cdots & 0 & 1
    \end{pmatrix}
  \end{displaymath}
\item Trouver une forme bilinéaire symétrique de $\R^n$ telle qu'il existe des vecteurs $u,v \in \R^n$ avec $\pscal{u,u}<0$ et $\pscal{v,v}>0$. 
\item Soit $V$ un espace vectoriel sur $\R$ muni d'une forme bilinéaire symétrique. S'il existe des vecteurs $u,v \in V$ tels que $\pscal{u,u}<0$ et $\pscal{v,v}>0$, il existe un vecteur $w \neq 0$ tel que $\pscal{w,w}=0$. 
\item Montrer que l'inégalité de Bessel (Théorème \ref{thr:4}) est une égalité si $v$ est dans le sous-espace engendré par les $v_1,\dots,v_n$. 
\item On considère l'espace euclidien des fonctions continues sur l'intervalle $[0,1]$ muni de la forme bilinéaire symétrique 
  \begin{displaymath}
    \pscal{f,g} = \int_0^1 f(x)g(x) \, dx. 
  \end{displaymath}
  \begin{enumerate}[i)]
  \item Soit $V$ le sous-espace engendré par $f(x) = x$ et $g(x) = x^2$. Trouver une base orthonormale de $V$. 
  \item Soit $V$ le sous-espace engendré par $\{1,x,x^2\}$. Trouver une base orthonormale de $V$. 
  \end{enumerate}
\item Soient $V$ un espace euclidien, $\{u_1,\dots,u_n\}$ un ensemble orthonormal et $f,g \in \spa\{u_1,\dots,u_n\}$. Montrer l'\emph{identité de Parseval}
  \begin{displaymath}
    \pscal{f,g} = \sum_i \pscal{f,u_i}\pscal{u_i,g}. 
  \end{displaymath}
\end{enumerate}


\section{La méthode des moindres carrées} 
\label{sec:le-methode-des}

Soient $A \in \R^{m \times n} $ et $b \in \R^m$ et supposons  que le système des équations linéaires 
\begin{equation}
  \label{eq:2}
  Ax = b
\end{equation}
n'a pas de solution. Dans beaucoup d'applications, on cherche un $x \in \R^n$ tel que la distance entre  $Ax$ et $b$ est \emph{minimale}. On aimerait alors résoudre le problème d'optimisation suivant
\begin{equation}
  \label{eq:3}
  \min_{x \in \R^n} \|Ax - b\|. 
\end{equation}
Ici la norme $\|⋅\|$ est par rapport au produit scalaire standard de $ℝ^n$.  

% \begin{framed}\noindent 
%   Pour le reste de ce paragraphe~\ref{sec:le-methode-des}, s'il n'est pas spécifié autrement, $V$ est toujours un espace euclidien.
% \end{framed} 


\begin{lemma}
  \label{lem:26}
  Soit  $V$ un espace euclidien, $H ⊆ V$ un sous espace de $V$, $b ∈ V$ et $h ∈ H$ tel que $b-h ∈ H^⊥$, alors
  \begin{displaymath}
    \|b- h' \| >  \|b-h\| 
  \end{displaymath}
  pour tout $h' ∈ H$, $h ≠ h'$. En particulière $h$ est unique. 
\end{lemma}

\begin{proof}
  Soit $h' ≠ h ∈ H$. Puisque $h - h' ∈ H$, Pythagore  implique 
  \begin{eqnarray*}
    \| b - h' \|^2 & = & \| b - h + ( h- h') \|^2 \\
                   & = & \| b - h\| ^2  + \| ( h- h') \|^2\\ 
                   & > &       \| b - h\| ^2. 
  \end{eqnarray*}
  
\end{proof}

% \begin{theorem}
%   \label{thr:3}
%   Soient $v_1,\dots,v_n$ des vecteurs deux à deux orthogonaux et tels que $\|v_i\|>0$ pour tout $i$. Soit $v$ un élément de $V$ et soit $\alpha_i = \pscal{v,v_i}/\pscal{v_i,v_i}$ la composante de $v$ sur $v_i$. Pour $a_1,\dots,a_n \in \R$ alors 
%   \begin{displaymath}
%     \left\| v - \sum_{i=1}^n \alpha_iv_i \right\|  \leq \left\| v - \sum_{i=1}^n a_iv_i \right\|.
%   \end{displaymath}
% De plus, l'inégalité au-dessus est une égalité si et seulement si $a_i = \alpha_i$ pour tout $i$. 
% Alors $\sum_{i=1}^n \alpha_iv_i$ est l'unique  meilleure approximation de $v$ par un vecteur du sous-espace engendré par les $v_1,\dots,v_n$. 
% \end{theorem}


% \begin{proof}
%   \begin{eqnarray*}
%     \|v- \sum_{i=1}^n a_iv_i \|^2 & = &  \|v - \sum_{i=1}^n \alpha_iv_i  - \sum_{i=1}^n (a_i - \alpha_i)v_i \|^2 \\
%                            & = & \|v - \sum_{i=1}^n \alpha_iv_i \|^2 + \| \sum_{i=1}^n (a_i - \alpha_i)v_i \|^2
%   \end{eqnarray*}
% en utilisant le lemme~\ref{lem:2} et le théorème de Pythagore. 
% \end{proof}

\noindent 
Maintenant nous pouvons décrire un \emph{algorithme} pour résoudre le problème suivant. 
\begin{framed}
  \noindent 
  Soient $b,a_1,\dots,a_n \in V$, trouver $h \in H = \spa \{a_1,\dots,a_n\}$ tel que la distance 
  \begin{displaymath}
    \|b - h\|
  \end{displaymath}
  est minimale. 
\end{framed}

\begin{algorithm}
\label{alg:2}

~\\
\begin{enumerate}[i)]
\item Trouver une base orthonormale $\{u_1,\dots,u_k\}$ du sous-espace $H = \spa\{a_1,\dots,a_n\}$ 
  avec le procédé de Gram-Schmidt. 
\item Retourner $ h = \sum_{i=1}^k \pscal{b,u_i} u_i$. 
\end{enumerate}
\end{algorithm}
\noindent 
Pour $b ∈ℝ^m$ et $a_1,\dots,a_n ∈ ℝ^m$ étant les colonnes des $A ∈ ℝ^{m ×n}$ en \eqref{eq:2},  une solution
$x^* ∈ℝ^n$ de
\begin{equation}
  \label{eq:53}
  Ax = h
\end{equation}
est une solution optimale de \eqref{eq:3}. Une procédé plus simple est comme suivant. 


\begin{theorem}
  \label{thr:6}
  Soient $A \in \R^{m\times n}$ et $b \in \R^m$. Les solutions du système 
  \begin{equation}
    \label{eq:4}    
    A^TAx = A^T b. 
  \end{equation}
  sont les solutions optimales du problème \eqref{eq:3} (où l'on considère la norme euclidienne sur $\R^m$, qui est la norme induite par le produit scalaire ordinaire)
\end{theorem}

\begin{proof}
  Soit $H =  \spa\{a_1,\dots,a_n\}$.
  Soit  $x^* ∈ ℝ^n $ et   $h = Ax^*$. Le vecteur $b- h$ est orthogonal a tout les colonnes de $A$
  (et alors à $H$) si et seulement si $0 = A^T (b-h) = A^T b - A^T A x^*$.  

 L'assertion est donc une conséquence du Lemme~\ref{lem:26}. 
\end{proof}

\begin{remark}
Pour une norme $\lvert\lvert \cdot \rvert\rvert$ quelconque engendrée par un produit scalaire $\langle \cdot , \cdot \rangle$), une preuve similaire montre que les solutions du système
  \begin{equation*}  
    A^TF^{\langle \cdot , \cdot \rangle}Ax = A^T F^{\langle \cdot , \cdot \rangle} b
  \end{equation*}
  sont les solutions  optimales du problème \eqref{eq:3}, où $F^{\langle \cdot , \cdot \rangle}$ est la matrice du produit scalaire $\langle \cdot , \cdot \rangle$ selon la base canonique (i.e. $(F^{\langle \cdot , \cdot \rangle})_{i,j} = \langle e_i , e_j \rangle$).
\end{remark}

\begin{example}
\label{exe:4}
Trouver une solution de moindre carrées sur les données 
\begin{displaymath}
  A =
  \begin{pmatrix}
    4 & 0 \\ 0 &2 \\ 1 & 1
  \end{pmatrix}
\text{ et }  b =
\begin{pmatrix}
  2\\0\\11
\end{pmatrix}
\end{displaymath}  

\begin{displaymath}
  A^T A =
  \begin{pmatrix}
    17 & 1 \\
    1 & 5
  \end{pmatrix} \text{ et } A^T b =
  \begin{pmatrix}
    19 \\ 11
  \end{pmatrix}. 
\end{displaymath}
La solution du système 
\begin{displaymath}
   \begin{pmatrix}
    17 & 1 \\
    1 & 5
  \end{pmatrix}
  \begin{pmatrix}
    x_1\\x_2
  \end{pmatrix}
= \begin{pmatrix}
    19 \\ 11
  \end{pmatrix}.
\end{displaymath}
est $x^* = (1,2)^T$. 
\end{example}








\section{Formes linéaires, bilinéaires et l'espace dual}
\label{sec:lespace-dual}
\todo{Chapitre~\ref{sec:lespace-dual} ne fait pas parie du cours printemns 2024} 


Soient $V$ un espace vectoriel sur un corps $K$ et $V^*$ l'ensemble  des  applications linéaires de $V$ dans $K$, où on considère $K$ comme espace vectoriel de dimension $1$ sur lui-même. Clairement, $V^*$ est un espace vectoriel lui-même. 
\begin{definition}
\label{def:9}
  L'ensemble des  applications linéaires  $φ : V ⟶ K$ est noté $V^*$ et, muni de l'addition et de la multiplication scalaire usuelles, est appelé l'\emph{espace dual} de $V$. Les éléments de $V^*$ sont appelés \emph{formes linéaires}.
\end{definition}

\begin{remark}
  \label{def:10}
  Soit $V$ un espace vectoriel sur un corps $K$. Une application 
  \begin{displaymath}
    f\colon V \times V \longrightarrow K
  \end{displaymath}
  est une forme bilinéaire si et seulement si pour tout $v \in V$,
  les applications $g,h: V \longrightarrow K$
  telles que $g(x) = f(v,x)$
  et $h(x) = f(x,v)$ sont des formes linéaires.
\end{remark}

Si $V$ est de dimension finie et si $B = \{v_1,\dots,v_n\}$ est une base de $V$, l'image d'un vecteur $x = \sum_i\alpha_i v_i$ par une forme linéaire $f$ est 
\begin{eqnarray*}
    f(x) & = &  f\left(\sum_i\alpha_i v_i\right) \\
         & = & \sum_i \alpha_i f(v_i)  \\
         & = & (f(v_1),\dots,f(v_n)) [x]_B,               
\end{eqnarray*}
où $[x]_B = (\alpha_1,\dots,\alpha_n)^T$ sont les coordonnées de $x$ dans la base $B$.  





\begin{lemma}
  \label{lem:3} Supposons que $V$ est de dimension finie et $\{v_1,\dots,v_n\}$ est une base de $V$. 
  La fonction $\phi_j\colon V \longrightarrow K$ 
    \begin{displaymath}
  \phi_j \left(\sum_i \alpha_i v_i \right) =  \alpha_j 
\end{displaymath}
est une forme linéaire. 
\end{lemma}
\begin{proof}
  Immédiate. 
\end{proof}

\begin{theorem}
  \label{thr:7}
  Les formes linéaires $\{\phi_j \in V^* \colon j=1,\dots,n\}$ du lemme~\ref{lem:3} précédent  forment une base de $V^*$.
\end{theorem}

\begin{proof}
  Si, pour $\beta_i \in K$, on a $\sum_i \beta_i \phi_i = 0$, alors 
  \begin{displaymath}
 0 =    \left(\sum_i \beta_i \phi_i\right) (v_j) =  \beta_j,
  \end{displaymath}
c'est-à-dire  les $\phi_j$ sont linéairement indépendantes. Les $\phi_j$ engendrent $V^*$ puisque pour $f \in V^*$, $f = \sum_i  f(v_i)  \, \phi_i$. 
\end{proof}

\begin{definition}
  \label{def:14}
  La base $\{\phi_1,\dots,\phi_n\}$ est la \emph{base duale} de la base $\{v_1,\dots,v_n\}$. 
\end{definition}




\begin{lemma}
  \label{lem:5}
  Soit $V$
  un espace vectoriel sur le corps $K$
  de dimension finie muni d'une forme bilinéaire  non dégénérée et soit
  $f\colon V \longrightarrow K$
  une forme linéaire. Il existe un $v \in V$
  tel que $f(x) = \pscal{v,x}$ pour tout $x \in V$.
\end{lemma}

\begin{proof}
  Soient $B = \{v_1,\dots,v_n\}$ une base  de $V$ et $A_B^{\pscal{}} \in K^{n\times n}$ la matrice dans la base $B$ associée à la forme bilinéaire. Soit $a \in K^n$ tel que  $f(x) = a^T[x]_B$ pour tout $x \in V$. Dès que $A_B^{\pscal{}}$ est de rang plein (Proposition~\ref{prop:7}), alors il existe $v \in V$ tel que $[v]_B^TA_B^{\pscal{}} = a^T$. Ceci revient à résoudre un système d'équations linéaires (cf semestre 1) et comme la matrice $A_B^{\pscal{}}$ est de rang plein, on a l'existence (et même l'unicité) d'une solution, i.e.,  $[v]_B^TA_B^{\pscal{}} = a^T$. Ainsi 
$f(x) = \pscal{v,x}$ pour tout $x \in V$. 
\end{proof}




\begin{theorem}[Supplémentaire orthogonal]
  \label{thr:8}
  Soient $V$ un espace vectoriel de dimension finie sur corps $K$ et $W$ un sous-espace de $V$. Soit $\pscal{.}$ une forme bilinéaire symétrique tel que, si restreint sur $W\times W$, elle est non dégénérée. Alors $V = W \oplus W^\perp$.  
\end{theorem}

\begin{proof}
  Pour un élément $u \in W \cap W^\perp$ on a $\pscal{u,w} = 0$ pour tout $w \in W$. Dès que $\pscal{.}$ est non dégénéré sur $W$ on a $u=0$, alors $W \cap W^\perp = \{0\}$. 

Il reste à démontrer que $V = W + W^\perp$. Pour $v \in V$ le lemme~\ref{lem:5} implique qu'il existe un $w_0 \in W$ tel que pour tout $u \in W$,  $\pscal{u,v} = \pscal{u,w_0}$ et ça démontre   $v - w_0 \in W^\perp$ et alors $v \in W + W^\perp$. 
\end{proof}

\subsection*{Exercices} 

\begin{enumerate}
\item Soient $V$ un espace vectoriel de dimension finie,  $f: V \longrightarrow K$ une forme linéaire et $B,B'$ des bases de $V$. Soit 
  \begin{displaymath}
    f(x) = a^T [x]_B 
  \end{displaymath}
  où $a \in K^n$. 
  Décrire $f(x)$ en termes de $P_{B'B}$ et $[x]_{B'}$. 
\item Soient $V$ un espace vectoriel de dimension finie,  $f: V\times V \longrightarrow K$ une forme bilinéaire et $B,B'$ des bases de $V$. Soit 
  \begin{displaymath}
    f(x,y) = [y]_B ^T A_B^f [y]_B.  
  \end{displaymath}
  Décrire $f(x,y)$ en termes de $P_{B'B}$, $[x]_{B'}$, et $[y]_{B'}$. 
\item On considère les vecteurs 
  \begin{displaymath}
    v_1 =
    \begin{pmatrix}
      1\\1\\0\\0
    \end{pmatrix} \text{ et }
 v_2 =
    \begin{pmatrix}
      0\\1\\1\\0
    \end{pmatrix} \in \Z_2^4
  \end{displaymath}
et la forme bilinéaire standard. Trouver une base du $\spa\{v_1,v_2\}^\perp$. Est-ce que $\Z_2^4 = \spa\{v_1,v_2\} \oplus \spa\{v_1,v_2\}^\perp$? 
\item Soit $V \subseteq \R[x]$ l'espace euclidien des polynômes de degré au plus $n$ muni du produit scalaire  $\pscal{p,q} = \int_0^1p(x)q(x) \, dx$.  Décrire la matrice $A_B^{\pscal{}}$ pour $B = \{1,x,\dots,x^n\}$. 
\item Soit $V$ un espace vectoriel sur un corps $K$ et soient $f,g \in V^* \setminus \{0\}$ linéairement indépendants. Montrer que
  \begin{displaymath}
    \ker{f} \cap \ker{g} 
  \end{displaymath}
est de dimension $n-2$. 
\item Soit $V$ un espace vectoriel de dimension finie sur un corps $K$ et soit $\pscal{.}$ une forme bilinéaire symétrique. Exprimez $(W_1 +W_2)^\perp$ et $(W_1\cap W_2)^\perp$ en fonction de $W_1^\perp$ et $W_2^\perp$. 


\item Donner un exemple d'un espace vectoriel  $V$ muni d'un produit scalaire dégénéré et d'un sous-espace $W \subseteq V$  tel que $V$ n'est pas la somme directe de $W$ et $W^\perp$. 
\end{enumerate}








\section{Formes sesquilinéaires et produits hermitiens}
\label{sec:form-sesq-et}

Maintenant nous considérons le cas $K = ℂ$.
Il faut un peu modifier la définition d'un produit scalaire pour
obtenir des résultats similaires à ceux des sections précédentes.  Le
carré de la longueur d'un vecteur
\begin{displaymath}
  v =
  \begin{pmatrix}
    a_1 + i\cdot  b_1 \\
    \vdots \\
    a_n + i \cdot b_n
  \end{pmatrix} \in \C^n
\end{displaymath}
où $a_i,b_i \in \R$, est égal à 
\begin{displaymath}
  \sum_i (a_i^2 + b_i^2) = \sum_i (a_i + i b_i) \cdot (a_i - i b_i) = \sum_i v_i \cdot \overline{v_i}, 
\end{displaymath}
où $v_i = a_i + i \cdot b_i$ et $\bar{v_i}$ est la conjugaison de $v_i$. Ceci suggère la définition suivante. 

\begin{definition}
  \label{def:15}
  Soit $V$
  un espace vectoriel sur un corps $\C$ et 
  \begin{displaymath}
    \pscal{,} : V × V ⟶ ℂ 
  \end{displaymath}
  une correspondance qui à tout couple $(v,w)$
  d'éléments de $V$
  associe un nombre complexe, noté $\pscal{v,w}$. 
  En considérant 
  les propriétés suivantes: \medskip
  \begin{enumerate}[PH 1]
  \item On a $\pscal{v,w} = \overline{\pscal{w,v}}$ pour tout $v,w \in V$.  \label{ph1}
  \item Si $u,v$ et $w$ sont des éléments de $V$,  \label{ph2}
    \begin{displaymath}
      \pscal{u,v+w} = \pscal{u,v}+\pscal{u,w} \, \text{ et } \,  \pscal{v+w,u} = \pscal{v,u}+\pscal{w,u}
    \end{displaymath}
  \item Si $x \in \C$ et $u,v \in V$,  \label{ph3}
    \begin{displaymath}
       \pscal{x \cdot u , v} = x \pscal{u,v} \, \text{ et } \,   \pscal{u , x \cdot v} = \overline{x}\cdot  \pscal{u,v}.
    \end{displaymath}  
  \end{enumerate}
  \noindent
on dit que $\pscal{,}$ est 
\begin{enumerate}[i)]
\item une \emph{forme sesquilinéaire}, si $\pscal{,}$ satisfait 
  PH~\ref{ph2} et PH~\ref{ph3}. 
\item une \emph{forme hermitienne}, si $\pscal{,}$ satisfait 
  PH~\ref{ph1}, PH~\ref{ph2} et PH~\ref{ph3}. 
\item un \emph{produit hermitien}, si   $\pscal{,}$ satisfait 
  PH~\ref{ph1}, PH~\ref{ph2} et PH~\ref{ph3} et 
  \begin{displaymath}
    \pscal{v,v} >0, \text{ pour tout } v \in V \setminus \{0\}. 
  \end{displaymath}
\end{enumerate}

Une forme  sesquilinéaire  est \emph{non dégénérée à gauche} si la condition suivante est vérifiée. 
\begin{quote}
  Si $v \in V$ et si $\pscal{v,w}=0$ pour tout $w \in V$, alors $v = 0$. 
\end{quote}

\end{definition}


\begin{remark}
Si $\pscal{,} $  est une forme hermitienne, 
pour tout $v \in V$, on a $\pscal{v,v} \in \R$ dès que $\pscal{v,v} = \overline{\pscal{v,v}}$ par PH~\ref{ph1}.  On dit que la forme hermitienne   est \emph{définie positif} si $\pscal{v,v} >0$ pour tout $v \in V \setminus\{0\}$. Alors un produit hermitien est une forme hermitienne définie positif. 
\end{remark}

\begin{example}
  \label{exe:12}
  Le produit \emph{hermitien standard} de $\C^n$ 
  \begin{displaymath}
    \pscal{u,v} = \sum_i u_i \overline{v_i}
  \end{displaymath}
  satisfait les condition PH~\ref{ph1}-\ref{ph3} et est défini positif.  
\end{example}



Les notions d'\emph{orthogonalité, de perpendicularité, de base orthogonale} et \emph{de supplémentaire orthogonal}  sont définies comme avant. Aussi les  \emph{coefficients de Fourier} sont les mêmes.
Soit $w ∈ V$, $w ≠0$ et $v ∈V$. On cherche $α ∈ ℂ$ satisfaisant
\begin{displaymath}
  〈 w, v - α w 〉 = 0. 
\end{displaymath}
On trouve $\wb{α} = 〈w,v〉 / 〈 w,w〉$. Puisque $〈 w,w〉 ∈ ℝ$ et  $\wb{〈w,v〉} =  〈v,w〉$, alors
\begin{displaymath}
  α = 〈v,w〉 / 〈 w,w〉. 
\end{displaymath}



\begin{example}
  \label{exe:13}
  Soit $V$ l'espace vectoriel des fonctions $f\colon \R \longrightarrow \C$   continues sur l'intervalle $[0, 2\pi]$. Pour $f,g \in V$ on pose
  \begin{displaymath}
    \pscal{f,g} = \int_0^{2\pi} f(x) \overline{g(x)} \, dx.
  \end{displaymath}
C'est un produit hermitien défini positif. Les fonctions $f_n (x)=  e^{inx}$ pour $n \in \Z$ sont orthogonales dès que 
\begin{displaymath}
  f_n(x) \overline{f_m(x)} = e^{i(n-m)x} = \cos((n-m) x) + i \cdot \sin((n-m) x)
\end{displaymath}
et alors 
\begin{displaymath}
  \pscal{f_n,f_n} = \int_0^{2 \pi} 1 \, dx = 2 \pi 
\end{displaymath}
et pour $n \neq m$ 
\begin{displaymath}
  \pscal{f_n,f_m} = \int_0^{2 \pi} \cos( (n-m) x) \, dx  + i \cdot \int_0^{2 \pi} \sin( (n-m) x) \, dx = 0.
\end{displaymath}
Pour $f \in V$ la composante de $f$ sur $f_n$, ou le coefficient de Fourier de $f$ relativement à $f_n$, est 
\begin{displaymath}
  \frac{\pscal{f,f_n}}{\pscal{f_n,f_n}} = \frac{1}{2\pi} \cdot \int_0^{2\pi} f(x) e^{-inx} \, dx. 
\end{displaymath}
\end{example}





Soient $V$ un espace vectoriel sur $\C$ de dimension finie et $f: V \times V \longrightarrow \C$ une forme sesquilinéaire. Pour une base  $B = \{v_1,\dots,v_n\}$ de $V$ et $x = \sum_i \alpha_i v_i$ et $y = \sum_i \beta_i v_i$ on a 
\begin{displaymath}
  \pscal{x,y} = \sum_{ij} \alpha_i\overline{\beta_j} f(v_i,v_j)
\end{displaymath}
et avec la matrice $A_B^f = (f(v_i,v_j))_{1 \leq i,j \leq n}$   alors 
\begin{equation}
  \label{eq:9}
  \pscal{x,y} = [x]_B ^T A_B^{f} \overline{[y]_B} 
\end{equation}
où pour un vecteur $v \in \C^n$ le vecteur $\overline{v}$ est tel que $(\overline{v})_i = \overline{(v_i)}$ pour tout $i$. Pour une matrice $A \in \C^{m \times n}$, $\overline{A} \in \C^{m \times n}$ est la matrice telle que 
$\left(\overline{A}\right)_{ij} = \overline{(A_{ij})}$ pour out $i,j$. 
\begin{definition}
  \label{def:17}
  Une matrice $A \in \C^{n \times n}$ est appelée \emph{hermitienne} si on a 
  \begin{displaymath}
    A = \overline{A^T}. 
  \end{displaymath}
\end{definition}


\begin{proposition}
  \label{prop:3}
  Soit  $V$  un espace vectoriel sur $\C$ de dimension finie et soit $B$  une base de $V$. Une forme sesquilinéaire $f$ est une forme hermitienne si et seulement si $A_B^f$ est hermitienne.  
\end{proposition}


\begin{definition}
  \label{def:18}
  Deux matrices $A,B \in \C^{n \times n}$ sont \emph{congruentes complexes} s'il existe une matrice inversible $P \in \C^{n \times n}$ telle que $A = {P^T} \cdot B \cdot \overline{P}$. Nous écrivons $A \cong_\C B$.  
\end{definition}
$\cong_\C$ est aussi une relation d'équivalence. On peut aussi modifier l'algorithme~\ref{alg:1} tel qu'il calcule une matrice diagonale congruente complexe par rapport à une matrice hermitienne $A \in \C^{n \times n}$,  voir l'exercice~\ref{item:7}. Alors on a le théorème suivant. 

\begin{theorem}
  \label{thr:11}
  Soit $V$ un espace vectoriel sur $\C$ de dimension finie, muni d'une forme  hermitienne. Alors $V$ possède une base orthogonale. 
\end{theorem}
\begin{proof}
  Soit $B = \{v_1,\dots,v_n\}$  une base de $V$. Pour $x,y \in V$ on a 
  \begin{eqnarray*}
    \pscal{x,y} & = & [x]_B^T A_B^{\pscal{.}} \overline{[y]_B}. 
  \end{eqnarray*}
  Soit $P \in \C^{n\times n} $ inversible telle que
  \begin{displaymath}
    P^T A_B^{\pscal{.}} \overline{P} =
    \begin{pmatrix}
      c_1\\
      & \ddots \\
      && c_n
    \end{pmatrix}. 
  \end{displaymath}
  La base orthogonale est $w_1,\dots,w_n$ dont $ [w_j]_B$ est  la $j$-ème colonne de $P$, 
  \begin{displaymath}
     [w_j]_B =
  \begin{pmatrix}
    p_{1j}\\ \vdots \\ p_{nj}
  \end{pmatrix}, 
  \end{displaymath} 
alors $w_j = \sum_{i=1}^n p_{ij} v_i$. 

\end{proof}



\begin{example}
  \label{exe:16}
  On considère la matrice hermitienne 
  \begin{displaymath}
    A = \left[\begin{matrix}0 & - i & 3 + 4 i\\i & -2 & 12\\3 - 4 i & 12 & 5\end{matrix}\right]
  \end{displaymath}
et le but est de trouver une matrice inversible $P \in \C^{3 \times 3}$ telle que 
\begin{displaymath}
  P^T \cdot A \cdot \overline{P}
\end{displaymath}
est une matrice diagonale. Nous échangeons la première et la deuxième colonne ainsi que la première et la deuxième ligne et obtenons 
\begin{displaymath}
\left[\begin{matrix}-2 & i & 12\\- i & 0 & 3 + 4 i\\12 & 3 - 4 i & 5\end{matrix}\right]. 
\end{displaymath}
Après on transforme 
\begin{displaymath}
\left[\begin{matrix}1 & 0 & 0\\- 0.5 i & 1 & 0\\6 & 0 & 1\end{matrix}\right]\cdot  \left[\begin{matrix}-2 & i & 12\\- i & 0 & 3 + 4 i\\12 & 3 - 4 i & 5\end{matrix}\right]\cdot  
\left[\begin{matrix}1 & 0.5 i & 6\\0 & 1 & 0\\0 & 0 & 1\end{matrix}\right]
  = 
\left[\begin{matrix}-2 & 0 & 0\\0 & 0.5 & 3 - 2 i\\0 & 3 + 2 i & 77\end{matrix}\right]
\end{displaymath}
La prochaine transformation est 

\begin{displaymath}
  \left[\begin{matrix}1 & 0 & 0\\0 & 1 & 0\\0 & -6 - 4 i & 1\end{matrix}\right] \cdot 
\left[\begin{matrix}-2 & 0 & 0\\0 & 0.5 & 3 - 2 i\\0 & 3 + 2 i & 77\end{matrix}\right] \cdot 
\left[\begin{matrix}1 & 0 & 0\\0 & 1 & -6 + 4 i\\0 & 0 & 1\end{matrix}\right] = 
\left[\begin{matrix}-2 & 0 & 0\\0 & 0.5 & 0\\0 & 0 & 51\end{matrix}\right]. 
\end{displaymath}
Pour 
\begin{displaymath}
P =   \left[\begin{matrix}0 & 1 & 0\\1 & 0 & 0\\0 & 0 & 1\end{matrix}\right] \cdot 
\left[\begin{matrix}1 & - 0.5 i & 6\\0 & 1 & 0\\0 & 0 & 1\end{matrix}\right]
\cdot 
\left[\begin{matrix}1 & 0 & 0\\0 & 1 & -6 - 4 i\\0 & 0 & 1\end{matrix}\right]
\end{displaymath}
on obtient 
\begin{displaymath}
  P^T \cdot A \cdot \overline{P} = \left[\begin{matrix}-2 & 0 & 0\\0 & 0.5 & 0\\0 & 0 & 51\end{matrix}\right]. 
\end{displaymath}



\end{example}
 


\subsection*{Exercices}

\begin{enumerate}
\item Soit $V$ un espace vectoriel sur $\C$ et $f\colon V\times V \longrightarrow \C$ une application satisfaisant les axiomes 

  \begin{enumerate}[i)] 
  \item On a $f({v,w}) = \overline{f({w,v})}$ pour tout $v,w \in V$.  
  \item Si $u,v$ et $w$ sont des éléments de $V$,  
    \begin{displaymath}
      f({u,v+w}) = f({u,v}) +f({u,w}) 
    \end{displaymath}
  \item Si $x \in \C$ et $u,v \in V$, 
    \begin{displaymath}
      f({x \cdot u , v}) = x f({u,v}). 
    \end{displaymath}  
  \end{enumerate}
Montrer que $f$ est une forme hermitienne. 
\item Soit $V$ un espace vectoriel sur $\C$ de dimension finie et $f: V \times V \longrightarrow \C$ une forme sesquilinéaire. Pour une base  $B = \{v_1,\dots,v_n\}$ de $V$ et $x = \sum_i \alpha_i v_i$ et $y = \sum_i \beta_i v_i$ montrer en détail que 
  \begin{displaymath}
  \pscal{x,y} = [x]_B ^T A_B^{f} \overline{[y]_B} 
\end{displaymath}
avec la matrice $A_B^f = (f(v_i,v_j))_{1 \leq i,j \leq n}$. Indiquez l'application des axiomes PH~\ref{ph2}) et PH~\ref{ph3}) dans les pas correspondants. 
\item Démontrer la proposition~\ref{prop:3}. 
\item Soit $V$ un espace vectoriel sur $\C$ de dimension $n$ et soit $\pscal{.}$ une forme sesquilinéaire. Montrer que $\pscal{.}$ est non dégénéré si et seulement si $\rank(A_B^{\pscal{.}}) = n$  pour chaque base $B$ de $V$. 
\item Montrer que $\cong_\C$ est une relation d'équivalence. \label{item:6}
\item Modifier l'algorithme~\ref{alg:1} afin qu'il calcule une matrice diagonale congruente complexe par rapport à une matrice hermitienne $A \in \C^{n \times n}$. \label{item:7}
\item Soient $V$ un espace vectoriel sur $\C$ et $\dim(V) = 3$ et $B = \{v_1,v_2,v_3\}$ une base de $V$. Avec les matrices $A_i \in \C^{3\times 3}$ décrites en bas et les applications  $f_i(x,y) = [x]_B^T A_i \overline{[y]_B}$, cocher ce qui s'applique.  


\bigskip 

  \begin{center}
 
    \begin{tabular}{|c|c|c|c|}
      \hline 
      & $A_1$ & $A_2$ & $A_3$ \\\hline 
      forme sesquilinéaire & & & \\ \hline 
      forme hermitienne & & & \\ \hline 
    \end{tabular}
  \end{center}
  
  \medskip 


  \begin{displaymath}
    A_1 = 
    \begin{pmatrix}
      2 & 1 & 3 \\
      1 & 0 & 2 \\
      3 & 2 & 0
    \end{pmatrix}, \, 
    A_2 = 
 \begin{pmatrix}
      2 & 1+i & 3 \\
      1 & 0 & 2 \\
      3 & 2 & 0
    \end{pmatrix}, \, 
     A_3 = 
    \begin{pmatrix}
      2 & 1+ 2 \cdot i & 3 - i \\
      1 - 2 \cdot i & 0 & 2-i \\
      3-i & 2+i & 0
    \end{pmatrix}. 
  \end{displaymath}


  

\end{enumerate}





\section{Espaces hermitiens} 
\label{sec:espaces-hermitiens}



\begin{framed}\noindent 
  Pour le reste de ce paragraphe, s'il n'est pas spécifié autrement,  $V$
  est toujours un espace vectoriel sur $\C$
  muni d'un produit hermitien. Alors $V$ est un espace hermitien. 
\end{framed}




\begin{definition}
  \label{def:h5}
  Soit $\pscal{,}$ un produit hermitien. La \emph{longueur} ou la \emph{norme} d'un élément $v \in V$ est le nombre 
  \begin{displaymath}
    \| v \| = \sqrt{\pscal{v,v}}.
  \end{displaymath}
  Un élément $v \in V$ est un \emph{vecteur unitaire} si $\|v\| = 1$. 
\end{definition}

Aussi, l'inégalité de Cauchy-Schwarz est démontrée comme avant: 
\begin{equation}
  \label{eq:6}
  | \pscal{u,v}| \leq \|u\| \|v\|. 
\end{equation}


Les propriétés suivantes sont facilement vérifiées :
\begin{enumerate}[i)]
\item Pour tout $v \in V$, $\|v\|\geq 0$ et $\|v\| = 0$ si et seulement si $v = 0$. \label{item:8}
\item Pour $\alpha \in \C$ et $v \in V$ on a $\| \alpha \cdot v \| = |\alpha| \cdot \|v\|$. \label{item:9}
\item Pour chaque $u,v \in V$ $\|u+v\| \leq \|u\| + \|v\|$. \label{tr:2}
\end{enumerate}

Aussi, nous avons le théorème de Pythagore, l'inégalité de Bessel et la règle du parallélogramme.  
L'équivalent du procédé de Gram-Schmidt pour les espaces hermitiens est comme suit. 


\begin{theorem}[Le procédé d'orthogonalisation de Gram-Schmidt]
\label{thr:12}
  Soit $V$ un espace hermitien et  $\{v_1,\dots,v_n\} \subseteq V$
  un ensemble libre.  
  Il existe un ensemble libre orthogonal $\{u_1,\dots,u_n\}$
  de $V$
  tel que pour tout $i$,
  $\{v_1,\dots,v_i\}$
  et $\{u_1,\dots,u_i\}$ engendrent le même sous-espace de $V$.
\end{theorem}

Comme avant, une base orthonormale est une base orthogonale consistant
de vecteurs unitaires et le procédé de Gram-Schmidt nous donne le
corollaire suivant.

\begin{corollary}
  \label{co:6}
  Soit $V$
  un espace hermitien de dimension finie.  $V$
  possède alors une base orthonormale.
\end{corollary}



\subsection*{Exercices} 

\begin{enumerate}
\item (Composante de $u$
  sur $v$;
  indépendance de la direction) Soient $u,v \in V$
  tel que $\pscal{v,v} \neq 0$.
  Montrer qu'il existe un seul $α ∈ ℂ$ tel que
  \begin{displaymath}
    \pscal{u-α ⋅ v,v} = 0.
  \end{displaymath}
Pour ce $α$ on a 
\begin{displaymath}
  \pscal{v,u-α ⋅ v} =0. 
\end{displaymath}
\item Montrer l'inégalité de Cauchy-Schwarz. 
\item Montrer l'inégalité triangulaire~\ref{tr:2}). 
\item Montrer qu'un espace hermitien de dimension finie possède une base $B$ telle que $\pscal{x,y} = [x]_B \cdot [y]_B$, où $\cdot$ est le produit hermitien standard. \label{item:4}
\end{enumerate}


 
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
